{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visoutre/ai-notebooks/blob/main/Stable_Diffusion_Batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU52ZvES6-1T"
      },
      "source": [
        "## Stable Diffusion Batch by [visoutre](https://www.reddit.com/user/visoutre)\n",
        "\n",
        "Location for updates: [Stable Diffusion Batch.ipynb](https://colab.research.google.com/github/visoutre/ai-notebooks/blob/main/Stable_Diffusion_Batch.ipynb)\n",
        "\n",
        "<b> v 2.4.0, last updated 9/05/22 </b>\n",
        "<br>\n",
        "- Expand to view info...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Credits:**</u>\n",
        "- the setup work was done by [FutureArt](https://twitter.com/future__art) and [Pharmapsychotic](https://twitter.com/pharmapsychotic), credit goes to them for getting me started\n",
        "- I added the bulk of features useful to concept art and img2img\n",
        "- I used some public code to implement openCV facial recognition for face detailing\n",
        "- The workflow for the bonus **HD Generation** scripts was inspired by Inspired by [u/tokidokiyuki](https://www.reddit.com/user/tokidokiyuki/)\n",
        "[and their post on Reddit](https://www.reddit.com/r/StableDiffusion/comments/x45uk6/my_process_to_upscale_an_image_through_img2img/)\n",
        "- [uses Real ESRGAN for upscaling](https://github.com/xinntao/Real-ESRGAN)\n",
        "\n",
        "<u>**Terms of Use:**</u>\n",
        "- This code is free to use however you want. You may use it, modify it and do anything with it without crediting me for the code (although it would be nice to get give me a mention if you use this code)\n",
        "- Any images you create using this code fall under the terms set by [huggingface.co](https://huggingface.co/CompVis/stable-diffusion) , [Stability.Ai](https://stability.ai/) as well as Google Colab terms. You must use this technology ethically. I am not responsible for anything you create, nor do I own any of your creations\n",
        "- I recommend only using input images you own the copywrite to, otherwise you may be liable to copywrite infringement \n",
        "- Any text prompts I share in code or elsewhere are free for you to use however you want, unrestricted and without credit to me. I believe prompts should be public and shared freely\n",
        "- You may use any of your creations privately and commercially without credit\n",
        "- Any images I create with this code are owned by me and you may not use any of my examples for commercial use, especially my original art. Respect me for sharing this code freely\n",
        "\n",
        "<br> \n",
        "\n",
        "---\n",
        "\n",
        "<u>**Key Features Include:**</u>\n",
        " - process a single image or multiple images, single prompts or multiple prompts\n",
        " - randomizing starting keyword in the prompts for variations on a theme\n",
        " - processing sequential guidance scales and image strengths on a seed for data\n",
        " - randomizing guidance scales and image strengths for exploration\n",
        " - processing a list of prompts with a corresponding image for consistency\n",
        " - processing a list of prompts with a random image for creativity\n",
        " - save prompt templates which can be quickly loaded\n",
        " - cycle through multiple prompt templates with multiple input images/prompts\n",
        " - the prompt cycling feature are like having your own art design team who can create different styles or follow a style!\n",
        " - sequence animation with ability to warp from after # of frames\n",
        " - face recognition to upscale the faces\n",
        " - HD upscaling by splitting the image into tiles\n",
        "\n",
        "<u>**Upcoming Plans (todo list):**</u>\n",
        "  - port locally to use with an RTX / better GUI (long term)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "<u>**Changelog:**</u>\n",
        "\n",
        "v 2.2.2, last updated 9/05/22:\n",
        "- added Real ESRGAN to use by itself for upscaling images, or for use with face recognition\n",
        "- got the face recognition to work and it will output files to Google Drive\n",
        "- the final results of face recognition are not auto blended into the base image. How to do this cleanly is beyond me, so I just make use of masking the result with the base\n",
        "- don't plan to add more code soon since there's a lot of features to work with\n",
        "\n",
        "v 2.3.0, last updated 9/04/22:\n",
        "- added the Bonus feature; HD Generation. Creates a generation for a 512x512 square across an input image. Inspired by [u/tokidokiyuki](https://www.reddit.com/user/tokidokiyuki/)\n",
        "[and their post on Reddit](https://www.reddit.com/r/StableDiffusion/comments/x45uk6/my_process_to_upscale_an_image_through_img2img/)\n",
        "\n",
        "v 2.2.2, last updated 9/02/22:\n",
        "- cleaned up the documentation, cleaned up the sections and reordered some fields\n",
        "- added aspect ratio templates\n",
        "\n",
        "v 1 to 2:  updated between 8/27/22 and 9/01/22:\n",
        "- experimented with core features to assist with concept art & design\n",
        "- added interpolation, multiple input images, prompt templates, etc\n",
        "- began exploring advanced features with openCV such as using facial recognition to clean up faces automatically\n",
        "\n",
        "v 1.0.0, last updated 8/27/22:\n",
        "- began the Stable Diffusion journey with FutureArt and Pharmapsychotic's collab file as a starting point\n",
        "- my initial intention was to take their version that had multiple prompts and allow those prompts to correspond with their own unique input images\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<u>**How to download the Stable Diffusion model:**</u>\n",
        "\n",
        "- Visit https://huggingface.co/CompVis/stable-diffusion-v-1-4-original and agree to the terms and conditions.\n",
        "- Click the **Files and versions** tab\n",
        "- Click **stable-diffusion-v-1-4-original**\n",
        "- Click the **download** link where it says *This file is stored with Git LFS . It is too big to display, but you can still download it.*\n",
        "- If you have [Google Drive for desktop](https://www.google.com/drive/download/) (highly recommended), you can save it directly to your **AI/models** directory.\n",
        "  - Otherwise, download it and re-upload it to your [Google Drive](https://drive.google.com) in the **AI/models** directory. (This is risky, as the upload may time out.)"
      ],
      "metadata": {
        "id": "Kqeths1llBUO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GOr30k9Gh1L"
      },
      "source": [
        "# 1. <u> Setup (Run All) </u>\n",
        "- simply run the arrow here to set everything up, no need to open this section or mess with settings (time to set up: 3-4 mins)\n",
        "- make sure you have the Stable Diffusion model saved to your drive in AI/models/sd-v1-4.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uc5OwvKdjRJF",
        "outputId": "b13714ce-c5e4-461c-b475-69fe1205ff72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-484503e4-6d64-c69f-01b3-49822745b161)\n"
          ]
        }
      ],
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DeqQ7pt1zdI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1033f0ee-030c-4cec-9271-b06df60d86e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Outputs will be saved to /content/gdrive/MyDrive/AI/StableDiffusionBatch\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive and Prepare Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/StableDiffusionBatch\"\n",
        "!mkdir -p $outputs_path\n",
        "!mkdir -p \"init_image\"\n",
        "print(f\"Outputs will be saved to {outputs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XA1NNcxM724U"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "!pip install pytorch-lightning torch-fidelity\n",
        "!pip install numpy omegaconf einops kornia pytorch-lightning\n",
        "!pip install albumentations transformers\n",
        "!pip install ftfy jsonmerge resize-right torchdiffeq tqdm\n",
        "\n",
        "!git clone https://github.com/CompVis/stable-diffusion\n",
        "%cd stable-diffusion/\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append('./taming-transformers')\n",
        "sys.path.append('./k-diffusion')\n",
        "\n",
        "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n",
        "\n",
        "# Installation Setup for Facial Recognition when using Crop Upres Feature:\n",
        "!pip install opencv-python\n",
        "import cv2\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTjVEfsNlDly",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown You need to get the model weights yourself and put on Google Drive or this Colab instance\n",
        "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bcHsbr3hblrk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "bee999d40027416785d4698f7e3e988c",
            "77c264cdfb21453f8994daa2360d6054",
            "a4398d2cf4e94bb08eedd79535485bab",
            "12a9a30f474d44859f7389bed8a7ac0e",
            "72269156dce7432cab71dc93b93de244",
            "013f61fbb3044e02956773ac487bb03a",
            "4806fc725c6c4dfc9efc90a7b27c9167",
            "b324e263490544eb8d8fd463a88593b6",
            "71a8c096b78c4fb285790344e3773675",
            "e3101b0bb5fb43859dcd0214cf6095f6",
            "19cc9e6e5e2b48c2b044a3a2eb2c93c5",
            "da49cf804f3d40adbab13ec0ae554b46",
            "1ff40adbbc1a4eb4a286132eee09b029",
            "5463b99d75e54069be942114d54502fb",
            "df23800149dd42a18385a2198bad7846",
            "2e61374220f44c279ced59eb1617f4a9",
            "e586966c78764abbbf20b0ee12cb1e59",
            "c7faafbe10734dbbb88568e1fbcae3d5",
            "2d351119c30544b1bdc335250adf6b37",
            "55ebe9d5619d4f379d14ea87a531ef1f",
            "eeefff6f061a4fa2b65dfb85e359e234",
            "83499bca191140f28e191bccb48e1809",
            "4040a1d581fd432a8ea7f48801e406a2",
            "7d9a8dcde09640119a7133e71f7b7b06",
            "eec9abc1a09044a29705e657e32694b3",
            "2ef156e0ee7f47efa9025c0aa11a7a61",
            "7463f1a3ea674bf688104b934591ae51",
            "b852044834c2415a99dcf6f67b795101",
            "7ca9417a57144e26ba00472692022515",
            "8a94f5068da349799d0d8d9014399b56",
            "238e275ba7b44f4dabb9e61ba929d839",
            "f637b6cc5d684720a9ee62c37f1c925b",
            "e7a55be534164e31b4800460b741659b",
            "5a72913ff8ce4b21a87c10b992dd1b1a",
            "6ff886fbf15d43f19460cac8d83bd3b3",
            "3a8f4a589180403e83b2b5b9896d70a1",
            "a4cc559d014b42d3b500e42744bd35b8",
            "f427314ae73648a78abaadf8775e52b9",
            "aa55ac49ef4e4f53a3f141f910b25d6f",
            "d8451f010ae34ecd99001f8497c44084",
            "838792a8c3ae4dcf8fa40556a8cdc4e2",
            "633d9d004c0e441484331ada63fa7e02",
            "f8681474f4f342a5baa6526cf248a663",
            "50343766638c47e2b3b84d42aa22e9d7",
            "df2fbb7104ee4f55bbe3c4271418d00b",
            "997bad17e95a422dbc6db5b04f934fc3",
            "b0576877c08a48d3ae9bfa625b1d98f9",
            "90db2ecf78f84f0ba646797ab098bbd5",
            "d8071a44828d4b279e2570016a95c5d3",
            "311303c6a56c4bbf9c593db95818edc4",
            "076646f26d774e7c99acd2d77721d3bd",
            "24ef54aa93224d2ca8a152918443101a",
            "ce299766c2374cb7baeb4c813e1289ec",
            "d47ebcc69def47fea5c14d33736f7037",
            "672f185e14c047c399e19a5384206c7d",
            "5977579f7b4c48e5852761ccdb69fef2",
            "b9cd8451d45a421f9117455d56dd43e9",
            "caab461d98764eafb6688ee0556e04ee",
            "4a0b799401a248eb874ac1a4b9c8724e",
            "6396e08c4e5e42d8a96e630b1619581c",
            "7e8dac7d9dcb4341999b44cf3d26a06d",
            "b83b8f09613c4c10bbbfab63b52e6d7a",
            "e8254f3e8d9247f4bca280cce84127bf",
            "c7639c1bf1344380823ed5750d73db2e",
            "f3292434dee9462e93ed396085eb8134",
            "09a241563dd94966a4d1a7cc7faed52b"
          ]
        },
        "outputId": "6b9c66cd-54e0-4c03-c4ee-bd336f1b9947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\n",
            "Global Step: 470000\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/939k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bee999d40027416785d4698f7e3e988c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/512k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da49cf804f3d40adbab13ec0ae554b46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4040a1d581fd432a8ea7f48801e406a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a72913ff8ce4b21a87c10b992dd1b1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df2fbb7104ee4f55bbe3c4271418d00b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.59G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5977579f7b4c48e5852761ccdb69fef2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'logit_scale', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#@title Define helper functions\n",
        "\n",
        "import argparse, gc, json, os, random, sys, time, glob, requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import PIL\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from IPython.display import display, clear_output\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch.cuda.amp import autocast\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "#from init_image.dmdim import DDIMSampler # customer sampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "from k_diffusion.sampling import sample_lms\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "class config():\n",
        "    def __init__(self):\n",
        "        self.ckpt = checkpoint_model_file\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ddim_eta = 0.0\n",
        "        self.ddim_steps = 100\n",
        "        self.fixed_code = True\n",
        "        self.init_img = None\n",
        "        self.n_iter = 1\n",
        "        self.n_samples = 1\n",
        "        self.outdir = \"\"\n",
        "        self.precision = 'full' # 'autocast'\n",
        "        self.prompt = \"\"\n",
        "        self.sampler = 'klms'\n",
        "        self.scale = 7.5\n",
        "        self.seed = 42\n",
        "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.C = 4\n",
        "        self.f = 8\n",
        "      \n",
        "def load_img(path, w, h):\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        if os.path.isdir(path):\n",
        "            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n",
        "            path = os.path.join(path, random.choice(files))\n",
        "            print(f\"Chose random init image {path}\")\n",
        "        image = Image.open(path).convert('RGB')\n",
        "    image = image.resize((w, h), Image.LANCZOS)\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "opt = config()\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "\n",
        "def generate(opt):\n",
        "    global sample_idx\n",
        "    seed_everything(opt.seed)\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "\n",
        "    if opt.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    model_wrap = CompVisDenoiser(model)       \n",
        "    batch_size = opt.n_samples\n",
        "    prompt = opt.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "    init_latent = None\n",
        "\n",
        "    if opt.init_img != None and opt.init_img != '':\n",
        "        init_image = load_img(opt.init_img, opt.W, opt.H).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code and init_latent == None:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "\n",
        "    images = []\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for n in range(opt.n_iter):\n",
        "                    for prompts in data:\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        if init_latent != None:\n",
        "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,)\n",
        "                        else:\n",
        "\n",
        "                            if opt.sampler == 'klms':\n",
        "                                print(\"Using KLMS sampling\")\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
        "                                model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                                x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
        "                                extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
        "                                samples = sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                            else:\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                samples, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                                conditioning=c,\n",
        "                                                                batch_size=opt.n_samples,\n",
        "                                                                shape=shape,\n",
        "                                                                verbose=False,\n",
        "                                                                unconditional_guidance_scale=opt.scale,\n",
        "                                                                unconditional_conditioning=uc,\n",
        "                                                                eta=opt.ddim_eta,\n",
        "                                                                x_T=start_code)\n",
        "\n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        for x_sample in x_samples:\n",
        "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                            images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                            try:\n",
        "                              if opt.saveNumberPosition == False: # don't save the number at beginning of the file\n",
        "                                filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx:03})_{sample_idx:04} {opt.seed}.png\")#####\n",
        "                              else: # put the number at the beginning of the filename\n",
        "                                filepath = os.path.join(opt.outdir, f\"{sample_idx:04} {batch_name}({batch_idx:03}) {opt.seed}.png\")#####\n",
        "                            except:\n",
        "                                filepath = os.path.join(opt.outdir, f\"{sample_idx:04} {batch_name}({batch_idx:03}) {opt.seed}.png\")#####                              \n",
        "                            print(f\"Saving to {filepath}\")\n",
        "                            Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n",
        "                            sample_idx += 1\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this code to install Real-ESRGAN\n",
        "# this is a requirement for the bonus tools - face recognition\n",
        "\n",
        "#Before start, make sure that you choose:\n",
        "#* Runtime Type = Python 3\n",
        "#* Hardware Accelerator = GPU\n",
        "#in the **Runtime** menu -> **Change runtime type**\n",
        "#Then, we clone the repository, set up the envrironment, and download the pre-trained model.\n",
        "\n",
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "# Set up the environment\n",
        "!pip install basicsr\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "# Download the pre-trained model\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models"
      ],
      "metadata": {
        "cellView": "form",
        "id": "86Mo8iYbhQuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGldtYOMb5fk"
      },
      "source": [
        "# <u> Information on How to Use</u> *(DON'T RUN; this is Purely Informative / Documentation)*\n",
        "- this section only contains reminders and tips on usage. Section 2. Batch Prompting is the working code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gyim7jyfJ76",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc082a74-8bd7-4957-cefd-c190e0104564"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#@title Prompting:\\n#@markdown `NOTE: The settings in this informative cell are the code defaults for the most basic use.` \\n<br>\\n#@markdown `If you have issues or want to revert, set the fields in the code cell (inside Section 2. Batch Prompting) to these values`\\n<br>\\n#@markdown `It\\'s also best to make sure all the optional fields are set to 0, checked off, or none unless you intend to use the feature`\\n\\n\\n##1---------------------------------------------------------------------------------------------\\n#@markdown ---\\n#@markdown > <u>**Section 1): Saving/Folder Settings** *(required):* <br></u>\\n##@markdown Shared Prompt Ending:\\n#@markdown 1b) **`outputFolder`**\\n#@markdown - folder name where this session will store the images<br>\\n#@markdown - This folder will exist (by default) under content/gdrive/MyDrive/AI/StableDiffusionBatch/`\\n<br>\\n#@markdown 1b) **`saveSeperatePromptsToSubfolders`**\\n#@markdown - True = save each prompt in a seperate folder, first 3 strings in prompt = folder name\\n#@markdown - False = save all prompts results in the same outputFolder (defined above in 1a)<br>\\n<br>\\n1a) outputFolder = \"upres_portraits4\" #@param {type:\"string\"}\\n1b) saveSeperatePromptsToSubfolders = False#@param{type:\"boolean\"}\\n#@markdown ---\\n\\n\\n##2---------------------------------------------------------------------------------------------\\n#@markdown > <u>**Section 2): Prompt Template** *(optional):*</u>\\n<br>\\n#@markdown `Note: set to \\'none/custom\\' if you rather type your prompt details than use a preset. This is for convenience`\\n<br>\\n#@markdown `Note: adding a new preset requires these 3 steps: `\\n#@markdown - 1) adding a new line to the promptTemplateDict dictionary (number it correctly too)\\n#@markdown - 2) adding this prompt as a new drop down option in promptTemplate (in the cell code, line 31~)\\n#@markdown - 3) adding the dropdown text as an additional elif statement in the code (look at the pattern and repeat)\\n<br>\\n#@markdown 2a) **`promptTemplate`**\\n#@markdown - the core prompt templates. You can select a template to reuse the same starting word + ending strings used in the prompt\\n#@markdown - use these to save the inconvenience of copy/pasting the same text over and over again. focus on writing the subject instead\\n#@markdown - *Python structure -* **key: [\\'templateStartWord\\', \\'prompt details\\']** | example - 10: [\\'tattoo design of\\', \\'inking, drawing, detailed, art by artist\\']\\n\\n# in addition to writing the prompts here, you need to add them to the promptTemplate #@param in the main code, otherwise it won\\'t show up to select\\n\\npromptTemplateDict = {\\n0: [\\'\\', \\'none\\'],\\n<br>\\n#@markdown 2b) **`emotiveTemplate`**\\n#@markdown - similar to promptTemplate, these strings only go at the end of the prompt to give a consistent influence to results\\n#@markdown - example of use: if you want variants of all your character portraits to be zombies, or to be happy, use this feature\\n#@markdown - *disclaimer:* may not work as intended all the time (especially with long prompts; this text gets truncated\\n<br>\\n#@markdown 2c) **`randomEnd`**\\n#@markdown - text that goes at the end of all prompts. use comma to split the text and randomize between comma\\n<br>\\n\\n2a) promptTemplate = \\'none/custom\\' #@param [\"none/custom\",\"character - female adult\", \"character - male adult\", \"environment - scifi\", \"environment - fantasy\", \"vehicle - sketch\", \"vehicle - render\", \"style - anime keyframe (use high guidance scale of 15+)\", \"style - comic art inks\", \"style - blackwork / heavily inked tattoos\", \"fun - action figure character\", \"fun - stickers\", \"fun - marble statue\"]\\n2b) emotiveTemplate = \\'none\\' #@param [\"none\", \"happy\", \"angry\", \"sad\", \"zombie\", \"psychedelic\", \"sketchy drawing style\"]\\n2c) randomEnd = \"\" #@param {type:\"string\"}\\n#@markdown ---\\n\\n\\n\\n##3---------------------------------------------------------------------------------------------\\n#@markdown > <u>**Section 3): Prompt Details** *(optional - toggle on/off):* <br></u>\\n<br>\\n#@markdown `Note: Your prompt must be 77 tokens or less. Any longer and the extra text gets truncated` <br>\\n#<br>\\n#@markdown There\\'s many options for splitting up the prompts:<br>\\n#<br>\\n#@markdown - 3a): **`useThese3PromptFields`:** includes art styles or artist names *(ex. art by  alphonse mucha)*<br>\\n#<br>\\n#@markdown  - 3b): **`randomStart`:** an initial word that is randomly selected to go at the start of the prompt<br>\\n#@markdown    - I recommend 2 ways to use randomStart:<br>\\n#@markdown  - 1) use a list of multiple emotions or elements *(ex. icy, firey, rainy)*<br>\\n#@markdown  - 2) leave only a single option if you have a consistent theme *(ex. \\'female\\' or \\'male\\')*<br>\\n#<br>\\n#@markdown - 3c):**`midDetails`:** includes keywords to create higher detailed results *(ex. highly detailed, rendered in octane)*<br>\\n#<br>\\n#@markdown - 3d): **`endingStyle`:** includes art styles or artist names *(ex. art by  alphonse mucha)*<br>\\n#<br>\\n#@markdown `NOTE: you don\\'t have to split your prompts with these seperate fields. You can simply write the whole prompt in midDetails or in the batch prompt list too`<br>\\n#<br>\\n#<br>\\n#@markdown  Example of How Prompts Combine: \\n#@markdown - [randomStart] + **[batchPrompt]** + [midDetails] + [endingStyle] + [randomEnd]<br>\\n#@markdown  - `Example 1:` [pearlescent] **[jellyfish in space],** [digital art, concept art...], [by Alex Ross...]<br>\\n#@markdown  - `Example 2:` [pearlescent] **[Lamborghini],** [digital art, concept art...], [by Alex Ross...]<br>\\n#@markdown  - `Example 3:` [glowing] **[frogs on lily pads],** [digital art, concept art...], [by Alex Ross...]<br>\\n#@markdown  - `Example 4:` [sparkling] **[shells on a beach],** [digital art, concept art...], [by Alex Ross...]<br>\\n#@markdown - Notice in the examples the first word is random and the midDetail + endingStyle words are consistent\\n#@markdown - The batch prompts are set up in a list a seperate cell under the variable **promptsA**<br>\\n#<br>\\n\\n3a) useThese3PromptFields = True#@param{type:\"boolean\"}\\n3b) randomStart = \"female\" #@param {type:\"string\"}\\n3c) midDetails = \"digital art, concept art, smooth, sharp focus, high definition, rendered in Octane\" #@param {type:\"string\"}\\n3d) endingStyle = \"illustration, art by artgerm and greg rutkowski and alphonse mucha\" #@param {type:\"string\"}\\n#@markdown ---\\n\\n\\n##4---------------------------------------------------------------------------------------------\\n#@markdown > <u>**Section 4): Basic Settings** *(required - all modes):* </u><br>\\n<br>\\n#@markdown 4a) **`width and height`:**\\n#@markdown - image dimensions. range of 512-768 is best for portrait. Max is 704 x 704 depending on the GPU <br>\\n#<br>\\n#@markdown 4b) **`aspectRatio`:**\\n#@markdown - Presets for standard image ratios to quickly choose between. Print as well as Film Ratio standards<br>\\n#<br>\\n#@markdown 4c) **`guidance_scale`:**\\n#@markdown - strength of text prompt. 15 is good, 20 can have greater realism. 7 was beta default. Experiment<br>\\n\\n#<br>\\n#@markdown 4d) **`steps`:**\\n#@markdown - number of diffusion steps. more takes longer, but has better detail. test with 5-20 and use 50-80. rarely exceed 100 (costly and insignificant)<br>\\n#<br>\\n#@markdown 4e) **`number_of_repeats`:**\\n#@markdown - how many rounds of iterating through the prompt list. the prompts always run from first to last of each before repeating<br>\\n#<br>\\n#@markdown 4f) **`sampler`:**\\n#@markdown - it seems klms is default, but sometime of my results worked better with ddim, especially portraits<br>\\n#<br>\\n#@markdown 4g) **`seed`:**\\n#@markdown - set -1 to generate by random, otherwise input # seed to regenerate. Make use of randomStart/End with seed for fun variations, or use it for the sequence (same seed = consistent)<br>\\n#<br>\\n#@markdown 4h) **`first_prompt_only`:**\\n#@markdown - will only run on the first prompt in the prompts list<br>\\n#@markdown - good if you want to test on a single image without requiring the deletion of the list<br>\\n#<br>\\n4a) width = 640 #@param {type:\"integer\"}\\n4a) height = 640 #@param {type:\"integer\"}\\n4b) aspectRatio = \\'custom\\' #@param [\"custom\",\"1:1  (square)\",\"4:3  (standard)\",\"16:9 (widescreen)\",\"21:9 (ultrawide)\",\"4:5  (instagram)\",\"8x11 (paper standard)\",\"2:3  (poster standard)\"]\\nwidth_height = [width, height] # param{type: \\'raw\\'}\\n4c) guidance_scale = 15 #@param {type:\"slider\", min:0, max:40, step:0.5}\\n4d) steps = 50 #@param {type:\"integer\"}\\nsamples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\\nnumber_of_images = 1 #param {type:\"integer\"}\\n4e) number_of_repeats = 3 #@param {type:\"integer\"}\\n4f) sampler = \\'ddim\\' #@param [\"klms\",\"plms\", \"ddim\"]\\nddim_eta = 0.75 # param {type:\"number\"}\\n4g) seed = -1 # @param {type:\"integer\"}\\n4h) firstPromptOnly = False#@param{type:\"boolean\"}\\n#@markdown ---\\n\\n\\n##5---------------------------------------------------------------------------------------------\\n#@markdown > <u>**Section 5): Init Image (img2img)** *(optional):*</u><br> \\n<br>\\n#@markdown `Note: you must upload the image(s) to the init_image folder on the left Files menu under the / content/init_image/ folder, or change the defaultInitPath`<br>\\n#<br>\\n#@markdown 5a) **`randomizeImageFromFolder:`**\\n#@markdown  - leave unchecked (False) to go through the list of images that are lined up with the prompt\\n#@markdown  - set to True to pull a random image from the **defaultInitPath** to go with your prompts`<br>\\n#<br>\\n#@markdown - some notes on randomization:<br>\\n#@markdown  - if you leave randomization off, make sure your count of images matches the number of prompts in both lists 10 prompts = 10 images<br>\\n#@markdown  - if you randomize the init_images, then you don\\'t have to have the matching # of prompts to image. You could have 3 images in the folder and 100 prompts<br>\\n#<br>\\n#@markdown 5b) **`defaultInitPath:`**\\n#@markdown - where the init_images need to be saved to load. /content/ is fine most of the time\\n#@markdown - a custom drive path is great if you want to save collections on your drive to use randomly across different promps\\n<br>\\n#@markdown 5c) **`initial_image`:**\\n#@markdown - if you want to work with a single image without setting up the list array, then you can enter the image name/extension here<br>\\n#@markdown - otherwise you can leave this field blank<br>\\n#<br>\\n#@markdown 5d) **`init_strength`:** *IMPORTANT* I use this option the most *(since I use img2img the most)*<br>\\n#@markdown - Adjust the intensity of how much the input image influences the result<br>\\n#@markdown - 0 means no input image is used. range 0.1 - 0.3 follows loose for creativity. 0.45 - 0.7 follows image closer for consistency\\n#@markdown - The strength goes in increments of 0.1, however you will find that 0.05 is also a valid and different result in between 0 and 0.1!\\n<br>\\n5a) randomizeImageFromFolder = False#@param{type:\"boolean\"}\\n5b) defaultInitPath = \"/content/init_image/\" #@param {type:\"string\"}\\n5c) initial_image = \"\" #@param {type:\"string\"}\\ninit_image_or_folder = \"/content/init_image/\" + initial_image\\n5d) init_strength = 0 #@param {type:\"number\"}\\n#@markdown ---\\n\\n\\n\\n##6---------------------------------------------------------------------------------------------\\n#@markdown > <u>**Section 6): Sequence Settings - Cycles** *(Through Ranges - optional):* <br></u>\\n<br>\\n#@markdown -  `Note 1: check both init_strength_0_to_1 and cfg_scale_7_to_20 to True to generate a grid of possibilities. 80 generations`<br>\\n#@markdown -  `Note 2: Can iterate on a seed through the scales to see how the values affect results, or randomize:`\\n\\n#@markdown 6a) **`cycleThroughPromptList`:**\\n#@markdown - activate to cycle through the prompt list templates to generate **variations** on the same prompts\\n#<br>\\n#@markdown 6b) **`templatesToCycle`:**\\n#@markdown - when cycleThroughPromptList=True, these are the keys of the Prompt Templates to cycle through\\n#@markdown - example: 1,2 means a female and a male in a digital painting style is created for each prompt (granted you have default prompt templates)\\n#@markdown - the key numbers can be found labeled in the promptTemplateDict variable\\n#<br>\\n#@markdown 6c) **`init_strength_0_to_1`:**\\n#@markdown - Goes through image strength setting 0 - 1 to see its effects. runs only on 1st prompt\\n#<br>\\n#@markdown 6d) **`cfg_scale_7_to_20`:**\\n#@markdown -  Goes through guidance_scale setting 7 - 20 to see its effects. runs only on 1st prompt\\n#<br>\\n#@markdown 6e) **`init_strength_random`:**\\n#@markdown - randomly picks an image strength value from 0.1 to 0.7<br>\\n#<br>\\n#@markdown 6f) **`cfg_scale_random`:**\\n#@markdown - randomly picks an guidance_scale value from 7 to 20<br>\\n#<br>\\n6a) cycleThroughPromptList = False#@param{type:\"boolean\"}\\n6b) templatesToCycle = \"1,2\" #@param {type:\"string\"}\\n# these options would run 14 times:\\n6c) init_strength_0_to_1 = False#@param{type:\"boolean\"}\\n6d) cfg_scale_7_to_20 = False#@param{type:\"boolean\"}\\n6e) init_strength_random = False#@param{type:\"boolean\"}\\n6f) cfg_scale_random = False#@param{type:\"boolean\"}\\nsave_settings_file = False#param{type:\"boolean\"}\\n#@markdown ---\\n\\n\\n##7---------------------------------------------------------------------------------------------\\n#@markdown > <u>**Section 7): Sequence Settings with Images** *(optional interpolation):* <br></u>\\n<br>\\n#@markdown 7a) **`generatedImageCycle`:**\\n#@markdown - This will cycle through the initial prompt (if firstPromptOnly is True) or the list of prompts with slight variance, creating morphing images!\\n#@markdown - This can create trippy results OR be used to **create subtle variations for concept art / design!**\\n\\n#<br>\\n#@markdown 7b) **`specifyStrengthLock`:**\\n#@markdown - -1 is default to go from 0.6 to 0.2; instead you can lock to only gen say all 0.05 (note: It doesn\\'t blend as well locking to a single value. -1 is best)\\n#<br>\\n#@markdown 7c) **`usePromptsOverTimeCycle`:**\\n#@markdown - Set this to true to use the variable **promptInterpolateMulti** from cell 2\\n#@markdown - In the dictionary of promptInterpolateMulti, you\\'ll be able to set a duration of frames for each prompt and cycle through multiple prompts!\\n#@markdown - This option is fantastic for animated morphs over time. It can create trippy results and unpredictable transformations\\n#<br>\\n#@markdown 7d) **`promptOverTimeSetting03InsteadOf02`:**\\n#@markdown - Set this to True to generate interpolations down to 0.3 instead of 0.2. \\n#@markdown - This will create a more subtle interpolation which will blender better, but will not create as much interest\\n#<br>\\n# for interpolating using generated image cycles on INIT_STRENGTH:\\n7a) generatedImageCycle = False#@param{type:\"boolean\"} \\n7b) specifyStrengthLock = 0.2 #@param {type:\"number\"} # -1 is default to go from 0.6 to 0.2; or you can lock to only gen say 0.05. It doesn\\'t blend as well locking to a single value\\n7c) usePromptsOverTimeCycle = False#@param{type:\"boolean\"}\\n7d) promptOverTimeSetting03InsteadOf02 = False#@param{type:\"boolean\"}\\n#@markdown ---\\n\\n\\n##8---------------------------------------------------------------------------------------------\\n#@markdown > <u>**Section 8): Research Outputs** *(optional):* <br></u>\\n<br>\\n\\n\\n\\n\\n\\n#@markdown ---\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "'''\n",
        "#@title Prompting:\n",
        "#@markdown `NOTE: The settings in this informative cell are the code defaults for the most basic use.` \n",
        "<br>\n",
        "#@markdown `If you have issues or want to revert, set the fields in the code cell (inside Section 2. Batch Prompting) to these values`\n",
        "<br>\n",
        "#@markdown `It's also best to make sure all the optional fields are set to 0, checked off, or none unless you intend to use the feature`\n",
        "\n",
        "\n",
        "##1---------------------------------------------------------------------------------------------\n",
        "#@markdown ---\n",
        "#@markdown > <u>**Section 1): Saving/Folder Settings** *(required):* <br></u>\n",
        "##@markdown Shared Prompt Ending:\n",
        "#@markdown 1b) **`outputFolder`**\n",
        "#@markdown - folder name where this session will store the images<br>\n",
        "#@markdown - This folder will exist (by default) under content/gdrive/MyDrive/AI/StableDiffusionBatch/`\n",
        "<br>\n",
        "#@markdown 1b) **`saveSeperatePromptsToSubfolders`**\n",
        "#@markdown - True = save each prompt in a seperate folder, first 3 strings in prompt = folder name\n",
        "#@markdown - False = save all prompts results in the same outputFolder (defined above in 1a)<br>\n",
        "<br>\n",
        "1a) outputFolder = \"upres_portraits4\" #@param {type:\"string\"}\n",
        "1b) saveSeperatePromptsToSubfolders = False#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##2---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 2): Prompt Template** *(optional):*</u>\n",
        "<br>\n",
        "#@markdown `Note: set to 'none/custom' if you rather type your prompt details than use a preset. This is for convenience`\n",
        "<br>\n",
        "#@markdown `Note: adding a new preset requires these 3 steps: `\n",
        "#@markdown - 1) adding a new line to the promptTemplateDict dictionary (number it correctly too)\n",
        "#@markdown - 2) adding this prompt as a new drop down option in promptTemplate (in the cell code, line 31~)\n",
        "#@markdown - 3) adding the dropdown text as an additional elif statement in the code (look at the pattern and repeat)\n",
        "<br>\n",
        "#@markdown 2a) **`promptTemplate`**\n",
        "#@markdown - the core prompt templates. You can select a template to reuse the same starting word + ending strings used in the prompt\n",
        "#@markdown - use these to save the inconvenience of copy/pasting the same text over and over again. focus on writing the subject instead\n",
        "#@markdown - *Python structure -* **key: ['templateStartWord', 'prompt details']** | example - 10: ['tattoo design of', 'inking, drawing, detailed, art by artist']\n",
        "\n",
        "# in addition to writing the prompts here, you need to add them to the promptTemplate #@param in the main code, otherwise it won't show up to select\n",
        "\n",
        "promptTemplateDict = {\n",
        "0: ['', 'none'],\n",
        "<br>\n",
        "#@markdown 2b) **`emotiveTemplate`**\n",
        "#@markdown - similar to promptTemplate, these strings only go at the end of the prompt to give a consistent influence to results\n",
        "#@markdown - example of use: if you want variants of all your character portraits to be zombies, or to be happy, use this feature\n",
        "#@markdown - *disclaimer:* may not work as intended all the time (especially with long prompts; this text gets truncated\n",
        "<br>\n",
        "#@markdown 2c) **`randomEnd`**\n",
        "#@markdown - text that goes at the end of all prompts. use comma to split the text and randomize between comma\n",
        "<br>\n",
        "\n",
        "2a) promptTemplate = 'none/custom' #@param [\"none/custom\",\"character - female adult\", \"character - male adult\", \"environment - scifi\", \"environment - fantasy\", \"vehicle - sketch\", \"vehicle - render\", \"style - anime keyframe (use high guidance scale of 15+)\", \"style - comic art inks\", \"style - blackwork / heavily inked tattoos\", \"fun - action figure character\", \"fun - stickers\", \"fun - marble statue\"]\n",
        "2b) emotiveTemplate = 'none' #@param [\"none\", \"happy\", \"angry\", \"sad\", \"zombie\", \"psychedelic\", \"sketchy drawing style\"]\n",
        "2c) randomEnd = \"\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##3---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 3): Prompt Details** *(optional - toggle on/off):* <br></u>\n",
        "<br>\n",
        "#@markdown `Note: Your prompt must be 77 tokens or less. Any longer and the extra text gets truncated` <br>\n",
        "#<br>\n",
        "#@markdown There's many options for splitting up the prompts:<br>\n",
        "#<br>\n",
        "#@markdown - 3a): **`useThese3PromptFields`:** includes art styles or artist names *(ex. art by  alphonse mucha)*<br>\n",
        "#<br>\n",
        "#@markdown  - 3b): **`randomStart`:** an initial word that is randomly selected to go at the start of the prompt<br>\n",
        "#@markdown    - I recommend 2 ways to use randomStart:<br>\n",
        "#@markdown  - 1) use a list of multiple emotions or elements *(ex. icy, firey, rainy)*<br>\n",
        "#@markdown  - 2) leave only a single option if you have a consistent theme *(ex. 'female' or 'male')*<br>\n",
        "#<br>\n",
        "#@markdown - 3c):**`midDetails`:** includes keywords to create higher detailed results *(ex. highly detailed, rendered in octane)*<br>\n",
        "#<br>\n",
        "#@markdown - 3d): **`endingStyle`:** includes art styles or artist names *(ex. art by  alphonse mucha)*<br>\n",
        "#<br>\n",
        "#@markdown `NOTE: you don't have to split your prompts with these seperate fields. You can simply write the whole prompt in midDetails or in the batch prompt list too`<br>\n",
        "#<br>\n",
        "#<br>\n",
        "#@markdown  Example of How Prompts Combine: \n",
        "#@markdown - [randomStart] + **[batchPrompt]** + [midDetails] + [endingStyle] + [randomEnd]<br>\n",
        "#@markdown  - `Example 1:` [pearlescent] **[jellyfish in space],** [digital art, concept art...], [by Alex Ross...]<br>\n",
        "#@markdown  - `Example 2:` [pearlescent] **[Lamborghini],** [digital art, concept art...], [by Alex Ross...]<br>\n",
        "#@markdown  - `Example 3:` [glowing] **[frogs on lily pads],** [digital art, concept art...], [by Alex Ross...]<br>\n",
        "#@markdown  - `Example 4:` [sparkling] **[shells on a beach],** [digital art, concept art...], [by Alex Ross...]<br>\n",
        "#@markdown - Notice in the examples the first word is random and the midDetail + endingStyle words are consistent\n",
        "#@markdown - The batch prompts are set up in a list a seperate cell under the variable **promptsA**<br>\n",
        "#<br>\n",
        "\n",
        "3a) useThese3PromptFields = True#@param{type:\"boolean\"}\n",
        "3b) randomStart = \"female\" #@param {type:\"string\"}\n",
        "3c) midDetails = \"digital art, concept art, smooth, sharp focus, high definition, rendered in Octane\" #@param {type:\"string\"}\n",
        "3d) endingStyle = \"illustration, art by artgerm and greg rutkowski and alphonse mucha\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##4---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 4): Basic Settings** *(required - all modes):* </u><br>\n",
        "<br>\n",
        "#@markdown 4a) **`width and height`:**\n",
        "#@markdown - image dimensions. range of 512-768 is best for portrait. Max is 704 x 704 depending on the GPU <br>\n",
        "#<br>\n",
        "#@markdown 4b) **`aspectRatio`:**\n",
        "#@markdown - Presets for standard image ratios to quickly choose between. Print as well as Film Ratio standards<br>\n",
        "#<br>\n",
        "#@markdown 4c) **`guidance_scale`:**\n",
        "#@markdown - strength of text prompt. 15 is good, 20 can have greater realism. 7 was beta default. Experiment<br>\n",
        "\n",
        "#<br>\n",
        "#@markdown 4d) **`steps`:**\n",
        "#@markdown - number of diffusion steps. more takes longer, but has better detail. test with 5-20 and use 50-80. rarely exceed 100 (costly and insignificant)<br>\n",
        "#<br>\n",
        "#@markdown 4e) **`number_of_repeats`:**\n",
        "#@markdown - how many rounds of iterating through the prompt list. the prompts always run from first to last of each before repeating<br>\n",
        "#<br>\n",
        "#@markdown 4f) **`sampler`:**\n",
        "#@markdown - it seems klms is default, but sometime of my results worked better with ddim, especially portraits<br>\n",
        "#<br>\n",
        "#@markdown 4g) **`seed`:**\n",
        "#@markdown - set -1 to generate by random, otherwise input # seed to regenerate. Make use of randomStart/End with seed for fun variations, or use it for the sequence (same seed = consistent)<br>\n",
        "#<br>\n",
        "#@markdown 4h) **`first_prompt_only`:**\n",
        "#@markdown - will only run on the first prompt in the prompts list<br>\n",
        "#@markdown - good if you want to test on a single image without requiring the deletion of the list<br>\n",
        "#<br>\n",
        "4a) width = 640 #@param {type:\"integer\"}\n",
        "4a) height = 640 #@param {type:\"integer\"}\n",
        "4b) aspectRatio = 'custom' #@param [\"custom\",\"1:1  (square)\",\"4:3  (standard)\",\"16:9 (widescreen)\",\"21:9 (ultrawide)\",\"4:5  (instagram)\",\"8x11 (paper standard)\",\"2:3  (poster standard)\"]\n",
        "width_height = [width, height] # param{type: 'raw'}\n",
        "4c) guidance_scale = 15 #@param {type:\"slider\", min:0, max:40, step:0.5}\n",
        "4d) steps = 50 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "number_of_images = 1 #param {type:\"integer\"}\n",
        "4e) number_of_repeats = 3 #@param {type:\"integer\"}\n",
        "4f) sampler = 'ddim' #@param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 # param {type:\"number\"}\n",
        "4g) seed = -1 # @param {type:\"integer\"}\n",
        "4h) firstPromptOnly = False#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##5---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 5): Init Image (img2img)** *(optional):*</u><br> \n",
        "<br>\n",
        "#@markdown `Note: you must upload the image(s) to the init_image folder on the left Files menu under the / content/init_image/ folder, or change the defaultInitPath`<br>\n",
        "#<br>\n",
        "#@markdown 5a) **`randomizeImageFromFolder:`**\n",
        "#@markdown  - leave unchecked (False) to go through the list of images that are lined up with the prompt\n",
        "#@markdown  - set to True to pull a random image from the **defaultInitPath** to go with your prompts`<br>\n",
        "#<br>\n",
        "#@markdown - some notes on randomization:<br>\n",
        "#@markdown  - if you leave randomization off, make sure your count of images matches the number of prompts in both lists 10 prompts = 10 images<br>\n",
        "#@markdown  - if you randomize the init_images, then you don't have to have the matching # of prompts to image. You could have 3 images in the folder and 100 prompts<br>\n",
        "#<br>\n",
        "#@markdown 5b) **`defaultInitPath:`**\n",
        "#@markdown - where the init_images need to be saved to load. /content/ is fine most of the time\n",
        "#@markdown - a custom drive path is great if you want to save collections on your drive to use randomly across different promps\n",
        "<br>\n",
        "#@markdown 5c) **`initial_image`:**\n",
        "#@markdown - if you want to work with a single image without setting up the list array, then you can enter the image name/extension here<br>\n",
        "#@markdown - otherwise you can leave this field blank<br>\n",
        "#<br>\n",
        "#@markdown 5d) **`init_strength`:** *IMPORTANT* I use this option the most *(since I use img2img the most)*<br>\n",
        "#@markdown - Adjust the intensity of how much the input image influences the result<br>\n",
        "#@markdown - 0 means no input image is used. range 0.1 - 0.3 follows loose for creativity. 0.45 - 0.7 follows image closer for consistency\n",
        "#@markdown - The strength goes in increments of 0.1, however you will find that 0.05 is also a valid and different result in between 0 and 0.1!\n",
        "<br>\n",
        "5a) randomizeImageFromFolder = False#@param{type:\"boolean\"}\n",
        "5b) defaultInitPath = \"/content/init_image/\" #@param {type:\"string\"}\n",
        "5c) initial_image = \"\" #@param {type:\"string\"}\n",
        "init_image_or_folder = \"/content/init_image/\" + initial_image\n",
        "5d) init_strength = 0 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##6---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 6): Sequence Settings - Cycles** *(Through Ranges - optional):* <br></u>\n",
        "<br>\n",
        "#@markdown -  `Note 1: check both init_strength_0_to_1 and cfg_scale_7_to_20 to True to generate a grid of possibilities. 80 generations`<br>\n",
        "#@markdown -  `Note 2: Can iterate on a seed through the scales to see how the values affect results, or randomize:`\n",
        "\n",
        "#@markdown 6a) **`cycleThroughPromptList`:**\n",
        "#@markdown - activate to cycle through the prompt list templates to generate **variations** on the same prompts\n",
        "#<br>\n",
        "#@markdown 6b) **`templatesToCycle`:**\n",
        "#@markdown - when cycleThroughPromptList=True, these are the keys of the Prompt Templates to cycle through\n",
        "#@markdown - example: 1,2 means a female and a male in a digital painting style is created for each prompt (granted you have default prompt templates)\n",
        "#@markdown - the key numbers can be found labeled in the promptTemplateDict variable\n",
        "#<br>\n",
        "#@markdown 6c) **`init_strength_0_to_1`:**\n",
        "#@markdown - Goes through image strength setting 0 - 1 to see its effects. runs only on 1st prompt\n",
        "#<br>\n",
        "#@markdown 6d) **`cfg_scale_7_to_20`:**\n",
        "#@markdown -  Goes through guidance_scale setting 7 - 20 to see its effects. runs only on 1st prompt\n",
        "#<br>\n",
        "#@markdown 6e) **`init_strength_random`:**\n",
        "#@markdown - randomly picks an image strength value from 0.1 to 0.7<br>\n",
        "#<br>\n",
        "#@markdown 6f) **`cfg_scale_random`:**\n",
        "#@markdown - randomly picks an guidance_scale value from 7 to 20<br>\n",
        "#<br>\n",
        "6a) cycleThroughPromptList = False#@param{type:\"boolean\"}\n",
        "6b) templatesToCycle = \"1,2\" #@param {type:\"string\"}\n",
        "# these options would run 14 times:\n",
        "6c) init_strength_0_to_1 = False#@param{type:\"boolean\"}\n",
        "6d) cfg_scale_7_to_20 = False#@param{type:\"boolean\"}\n",
        "6e) init_strength_random = False#@param{type:\"boolean\"}\n",
        "6f) cfg_scale_random = False#@param{type:\"boolean\"}\n",
        "save_settings_file = False#param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##7---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 7): Sequence Settings with Images** *(optional interpolation):* <br></u>\n",
        "<br>\n",
        "#@markdown 7a) **`generatedImageCycle`:**\n",
        "#@markdown - This will cycle through the initial prompt (if firstPromptOnly is True) or the list of prompts with slight variance, creating morphing images!\n",
        "#@markdown - This can create trippy results OR be used to **create subtle variations for concept art / design!**\n",
        "\n",
        "#<br>\n",
        "#@markdown 7b) **`specifyStrengthLock`:**\n",
        "#@markdown - -1 is default to go from 0.6 to 0.2; instead you can lock to only gen say all 0.05 (note: It doesn't blend as well locking to a single value. -1 is best)\n",
        "#<br>\n",
        "#@markdown 7c) **`usePromptsOverTimeCycle`:**\n",
        "#@markdown - Set this to true to use the variable **promptInterpolateMulti** from cell 2\n",
        "#@markdown - In the dictionary of promptInterpolateMulti, you'll be able to set a duration of frames for each prompt and cycle through multiple prompts!\n",
        "#@markdown - This option is fantastic for animated morphs over time. It can create trippy results and unpredictable transformations\n",
        "#<br>\n",
        "#@markdown 7d) **`promptOverTimeSetting03InsteadOf02`:**\n",
        "#@markdown - Set this to True to generate interpolations down to 0.3 instead of 0.2. \n",
        "#@markdown - This will create a more subtle interpolation which will blender better, but will not create as much interest\n",
        "#<br>\n",
        "# for interpolating using generated image cycles on INIT_STRENGTH:\n",
        "7a) generatedImageCycle = False#@param{type:\"boolean\"} \n",
        "7b) specifyStrengthLock = 0.2 #@param {type:\"number\"} # -1 is default to go from 0.6 to 0.2; or you can lock to only gen say 0.05. It doesn't blend as well locking to a single value\n",
        "7c) usePromptsOverTimeCycle = False#@param{type:\"boolean\"}\n",
        "7d) promptOverTimeSetting03InsteadOf02 = False#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##8---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 8): Research Outputs** *(optional):* <br></u>\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGlb8rzxcB7F"
      },
      "source": [
        "# <u>Prompt Storage</u> *(reference only, don't run)*\n",
        "- this section is for saving useful prompts that aren't connected to code in any way, they are just here for convenience "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM2cByQdjN_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a7132f-021d-4515-ea02-db55749fc13b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCheesy Characters:\\ngothic, goth fantasy blacksmith apprentice with magical powers, character design, black hair, pentagram\\ngothic, goth fantasy blacksmith with magical powers, pentagram\\nbarbarian hunter with a bow and arrow and an athletic physique\\nwizard with a large brim magic hat and a long robe casting a spell\\nattractive queen of the moon Athena and a starry night\\nfemale gothic crusader with her face visible, long hair, metal armor, character design, full body\\n\\nFull Prompts:\\nfemale gothic crusader with her face visible, long hair, metal armor, character design, full body, intricate fine tipped pen drawing, inktober, Fine Line Tattoo, manga line art, monochrome, dotwork, by dan hilliard, by Stanislaw Wilczynski, by alphonse mucha, by aaron horkey\\n\\nAnimals:\\na frog relaxing on a lilypad while ribbiting\\n\\nPokemon:\\npokemon trainer portrait of a male forest ranger with a red hat and red jacket\\npokemon trainer portrait of a male jock with spiky black hair holding up a pokeball\\npokemon trainer portrait of a male electricity magician with bright yellow spiky hair juggling pokeballs\\npokemon trainer portrait of a creepy scientist with big glasses and a dumb expression, blue hair\\npokemon trainer portrait of a young boy who tries to look confident\\npokemon trainer portrait of a badass and cool pokemon trainer with street fashion, rapper cap, holding a pokeball\\n\\nInteresting Artists:\\nAlex Grey?? (psychedelic, trippy)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Put each of your prompts on a new line\n",
        "# Examples:\n",
        "'''\n",
        "Animals:\n",
        "a frog relaxing on a lilypad while ribbiting\n",
        "a shiba inu riding a ufo to the moon\n",
        "Characters:\n",
        "wizard with a large brim magic hat and a long robe casting a spell\n",
        "barbarian hunter with a club and an athletic physique\n",
        "'''\n",
        "\n",
        "# Prompt Template Brainstorming:\n",
        "'''\n",
        "extremely detailed oil painting, by rhads, sargent and leyendecker, savrasov levitan polenov, bruce pennington, studio ghibli, tim hildebrandt, digital art, landscape painting, trending on artstation, masterpiece\n",
        "a wholesome motorcycle ride across beautiful scenery, studio Ghibli, Pixar and Disney animation, sharp, Rendered in Redshift and Unreal Engine 5 by Greg Rutkowski, Bloom, dramatic lighting\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# my custom prompt ideas:\n",
        "'''\n",
        "Cheesy Characters:\n",
        "gothic, goth fantasy blacksmith apprentice with magical powers, character design, black hair, pentagram\n",
        "gothic, goth fantasy blacksmith with magical powers, pentagram\n",
        "barbarian hunter with a bow and arrow and an athletic physique\n",
        "wizard with a large brim magic hat and a long robe casting a spell\n",
        "attractive queen of the moon Athena and a starry night\n",
        "female gothic crusader with her face visible, long hair, metal armor, character design, full body\n",
        "\n",
        "Full Prompts:\n",
        "female gothic crusader with her face visible, long hair, metal armor, character design, full body, intricate fine tipped pen drawing, inktober, Fine Line Tattoo, manga line art, monochrome, dotwork, by dan hilliard, by Stanislaw Wilczynski, by alphonse mucha, by aaron horkey\n",
        "\n",
        "Animals:\n",
        "a frog relaxing on a lilypad while ribbiting\n",
        "\n",
        "Pokemon:\n",
        "pokemon trainer portrait of a male forest ranger with a red hat and red jacket\n",
        "pokemon trainer portrait of a male jock with spiky black hair holding up a pokeball\n",
        "pokemon trainer portrait of a male electricity magician with bright yellow spiky hair juggling pokeballs\n",
        "pokemon trainer portrait of a creepy scientist with big glasses and a dumb expression, blue hair\n",
        "pokemon trainer portrait of a young boy who tries to look confident\n",
        "pokemon trainer portrait of a badass and cool pokemon trainer with street fashion, rapper cap, holding a pokeball\n",
        "\n",
        "Interesting Artists:\n",
        "Alex Grey?? (psychedelic, trippy)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8hwXeJhIfJl"
      },
      "source": [
        "# 2. <u> Batch Prompting</u> *(where the magic takes place, **EXPAND THIS IT!**)*\n",
        "\n",
        "- Remember to run all 4 cells at the startup and the first 3 whenever you edit them!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Use Cases:** | txt2img (single or batch) | img2img (single or batch) | cycle through cfg/init | sequence animator | prompt templates\n",
        "Note: the next code cell after this line of text must be ran. it's where the descriptive prompt lists are stored"
      ],
      "metadata": {
        "id": "MMHp3PgL3knW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvU-T058oLsx"
      },
      "outputs": [],
      "source": [
        "##@title # CELL 2b: MAIN PROMPTS & INITIAL IMAGES (run)\n",
        "#Info = 'click Show Code to view, click Run Cell to update:' #@param [\"click Show Code to view, click Run Cell to update:\"]\n",
        "batch_idx = 0 # some users were getting errors for this and the number can get bloated after many generations, so this is a temp solution to avoid issues with it\n",
        "\n",
        "# IMPORTANT 1: Make sure to update this cell every time you change prompts and init_images!\n",
        "# IMPORTANT 2: For the prompts and images to match, make sure to sync their position in the list\n",
        "\n",
        "# Put each of your prompts on a new line:\n",
        "promptsA = '''\n",
        "a wholesome and cute cabin in the woods by Studio Ghibli\n",
        "athena the goddess of the night\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPfN1gYgoJkW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Cell 3: Main Code (the magic button; **RUN IT!**)\n",
        "##@markdown\n",
        "\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "# create a list of all the prompts from new line:\n",
        "if type(promptsA) is str:\n",
        "  promptsA = [i for i in promptsA.split('\\n') if i]\n",
        "\n",
        "def clean_string(s):\n",
        "    s = ''.join([c for c in s if (re.match('[a-zA-Z0-9 _]', c) or ord(c) > 127)]).strip()\n",
        "    if len(s) > 200:\n",
        "        return (s[:150]).strip()\n",
        "    return s\n",
        "\n",
        "##1---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 1): Saving/Folder Settings** *(required):* <br></u>\n",
        "outputFolder = \"folderNameInsert\" #@param {type:\"string\"}\n",
        "saveSeperatePromptsToSubfolders = False#@param{type:\"boolean\"}\n",
        "numberBeginningOfFile = True#@param{type:\"boolean\"}\n",
        "opt.saveNumberPosition = numberBeginningOfFile\n",
        "# note: the actual saving to output directory was moved lower since we might want to save the prompt name there\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##2---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 2): Prompt Template** *(optional):*</u>\n",
        "# <br>\n",
        "#@markdown `Note: set to none if you rather type your prompt details in section 3) |  the selected template is used on EVERY prompt in the promptsA list`\n",
        "promptTemplate = 'environment - fantasy' #@param [\"none/custom\",\"character - female adult\", \"character - male adult\", \"environment - scifi\", \"environment - fantasy\", \"vehicle - sketch\", \"vehicle - render\", \"design - weapon\", \"style - anime keyframe (use high guidance scale of 15+)\", \"style - comic art inks\", \"style - blackwork / heavily inked tattoos\", \"fun - action figure character\", \"fun - stickers\", \"fun - marble statue\"]\n",
        "templateSelected = \"none\"\n",
        "templateSelectedStart = \"\"\n",
        "templateSelectedKey = 0\n",
        "\n",
        "if promptTemplate == \"none/custom\":\n",
        "  templateSelectedKey = 0\n",
        "elif promptTemplate == \"character - female adult\":\n",
        "  templateSelectedKey = 1\n",
        "elif promptTemplate == \"character - male adult\":\n",
        "  templateSelectedKey = 2\n",
        "elif promptTemplate == \"environment - scifi\": \n",
        "  templateSelectedKey = 3\n",
        "elif promptTemplate == \"environment - fantasy\": \n",
        "  templateSelectedKey = 4\n",
        "elif promptTemplate == \"vehicle - sketch\": \n",
        "  templateSelectedKey = 5\n",
        "elif promptTemplate == \"vehicle - render\": \n",
        "  templateSelectedKey = 6\n",
        "elif promptTemplate == \"design - weapon\": \n",
        "  templateSelectedKey = 7\n",
        "elif promptTemplate == \"style - anime keyframe (use high guidance scale of 15+)\": \n",
        "  templateSelectedKey = 8\n",
        "elif promptTemplate == \"style - comic art inks\": \n",
        "  templateSelectedKey = 9\n",
        "elif promptTemplate == \"style - blackwork / heavily inked tattoos\": \n",
        "  templateSelectedKey = 10\n",
        "elif promptTemplate == \"fun - action figure character\": \n",
        "  templateSelectedKey = 11\n",
        "elif promptTemplate == \"fun - stickers\": \n",
        "  templateSelectedKey = 12\n",
        "elif promptTemplate == \"fun - marble statue\": \n",
        "  templateSelectedKey = 13\n",
        "\n",
        "# code for determining which key is selected:\n",
        "# templateSelectedKey = 6\n",
        "promptTemplatedSelectedlist = promptTemplateDict[int(templateSelectedKey)] # gets both the values of the selected key as a list, ex ['male', 'line art']\n",
        "templateSelectedStart = promptTemplatedSelectedlist[0]\n",
        "templateSelected = promptTemplatedSelectedlist[1]\n",
        "# end template prompt list ------------------------------------------------------------\n",
        "\n",
        "emotiveTemplate = 'none' #@param [\"none\", \"happy\", \"angry\", \"sad\", \"zombie\", \"psychedelic\", \"sketchy drawing style\"]\n",
        "emoteTemplateSelected = \"none\"\n",
        "if emotiveTemplate == \"none\":\n",
        "  emoteTemplateSelected = \"none\"\n",
        "\n",
        "elif emotiveTemplate == \"happy\":\n",
        "  emoteTemplateSelected = \"happy friendly cheerful smiling joy sparkling magical\"\n",
        "\n",
        "elif emotiveTemplate == \"angry\":\n",
        "  emoteTemplateSelected = \"villain angry pissed off frowning yelling spitting fighting violent extreme anger rage fury outrage furious open mouth\"\n",
        "\n",
        "elif emotiveTemplate == \"sad\":\n",
        "  emoteTemplateSelected = \"depressed sad saddened heartbroken pouting crying tears frown grief grieving hopeless whimpering\"\n",
        "\n",
        "elif emotiveTemplate == \"zombie\":\n",
        "  emoteTemplateSelected = \"black dark very dark black and creepy horror terrifying bloody gore wounds blood splats zombie terrified dead\"\n",
        "\n",
        "elif emotiveTemplate == \"psychedelic\":\n",
        "  emoteTemplateSelected = \"psychedelic trippy colorful neon kaleidoscope mushrooms trance bright mind-blowing mystical spiritual mysticism psilocybin hallucinogenic\"\n",
        "\n",
        "elif emotiveTemplate == \"sketchy drawing style\":\n",
        "  emoteTemplateSelected = \"line art line drawing pen and ink black and white simple simplified sketch drawing pencil drawing rough thin lines line linear etched flat drawing\"\n",
        "\n",
        "# end template emotive addition list -----------------------------------------------------------\n",
        "\n",
        "randomEnd = \"\" #@param {type:\"string\"}\n",
        "randomStringEnd = randomEnd.split(\",\")\n",
        "numOfRandomStringEnd = len(randomStringEnd)\n",
        "wordRNDMend = \"\"\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##3---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 3): Prompt Details** *(optional - toggle on/off):* <br></u>\n",
        "# <br>\n",
        "#@markdown `Note: these values are included on EVERY prompt in the list if no template is used and useThese3PromptFields = True`\n",
        "useThese3PromptFields = False#@param{type:\"boolean\"}\n",
        "randomStart = \"\" #@param {type:\"string\"}\n",
        "if useThese3PromptFields == False:\n",
        "  randomStart = \"\"\n",
        "wordRNDM = \"\"\n",
        "randomStringLS = randomStart.split(\",\")\n",
        "numOfRandomStringStart = len(randomStringLS)\n",
        "midDetails = \"\" #@param {type:\"string\"}\n",
        "endingStyle = \"\" #@param {type:\"string\"}\n",
        "if useThese3PromptFields == False:\n",
        "  midDetails = \"\"\n",
        "  endingStyle = \"\"\n",
        "# next we determine if we should actually use these custom strings or use our prompt templates:\n",
        "promptsB = \"\"\n",
        "if templateSelectedKey == 0:\n",
        "  promptsB = [x + \", \" + midDetails for x in promptsA] # add the universal style text to each prompt in the list\n",
        "  prompts = [x + \", \" + endingStyle for x in promptsB] # add the universal style text to each prompt in the list\n",
        "else:\n",
        "  prompts = [x + \", \" + templateSelected for x in promptsA] # add the universal style text to each prompt in the list\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##4---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 4): Basic Settings** *(required - all modes):* </u><br>\n",
        "#<br>\n",
        "#@markdown `Note: Defaults: width/height = 640/640 | scale = 15 | steps = 50 | repeats = 6 | sampler = ddim | seed = -1 (random)`\n",
        "width = 628 #@param {type:\"integer\"}\n",
        "height = 628 #@param {type:\"integer\"}\n",
        "aspectRatio = '1:1  (square)' #@param [\"custom\",\"1:1  (square)\",\"4:3  (standard)\",\"16:9 (widescreen)\",\"21:9 (ultrawide)\",\"4:5  (instagram)\",\"8x11 (paper standard)\",\"2:3  (poster standard)\"]\n",
        "if aspectRatio == \"custom\":\n",
        "  width = width\n",
        "  height = height\n",
        "elif  aspectRatio == \"1:1  (square)\":\n",
        "  width = 640\n",
        "  height = 640\n",
        "elif  aspectRatio == \"4:3  (standard)\":\n",
        "  width = 768\n",
        "  height = 576\n",
        "elif  aspectRatio == \"16:9 (widescreen)\":\n",
        "  width = 832\n",
        "  height = 480\n",
        "elif  aspectRatio == \"21:9 (ultrawide)\":\n",
        "  width = 1024\n",
        "  height = 448\n",
        "elif  aspectRatio == \"4:5  (instagram)\":\n",
        "  width = 512\n",
        "  height = 640\n",
        "elif  aspectRatio == \"8x11 (paper standard)\":\n",
        "  width = 512\n",
        "  height = 672\n",
        "elif  aspectRatio == \"2:3  (poster standard)\":\n",
        "  width = 512\n",
        "  height = 768\n",
        "\n",
        "width_height = [width, height] # param{type: 'raw'}\n",
        "guidance_scale = 16 #@param {type:\"slider\", min:0, max:40, step:1}\n",
        "steps = 40 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "number_of_images = 1 #param {type:\"integer\"}\n",
        "number_of_repeats = 10 #@param {type:\"integer\"}\n",
        "sampler = 'ddim' #@param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 # param {type:\"number\"}\n",
        "seed = -1 # @param {type:\"integer\"}\n",
        "firstPromptOnly = True#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##5---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 5): Init Image (img2img)** *(optional):*</u><br> \n",
        "#<br>\n",
        "#@markdown `Note: the default path is: / content/init_image/`\n",
        "randomizeImageFromFolder = False#@param{type:\"boolean\"}\n",
        "defaultInitPath = \"/content/init_image/\" #@param {type:\"string\"}\n",
        "if defaultInitPath == \"\":\n",
        "  defaultInitPath = \"/content/init_image/\"\n",
        "\n",
        "# find all the images in the folder meant to read them:\n",
        "numberOfImages = 0\n",
        "init_imgs_list = []\n",
        "for path in os.listdir(defaultInitPath):\n",
        "    if os.path.isfile(os.path.join(defaultInitPath, path)):\n",
        "        numberOfImages += 1\n",
        "        init_imgs_list.append(path)\n",
        "init_imgs_list.sort()\n",
        "# print(numberOfImages)\n",
        "# print(init_imgs_list)\n",
        "\n",
        "# create a list of all the init images from new line:\n",
        "if type(init_imgs_list) is str:\n",
        "  init_imgs_list = [i for i in init_imgs_list.split('\\n') if i]\n",
        "if len(init_imgs_list) == 0:\n",
        "  init_imgs_list.append(\"none\")\n",
        "  \n",
        "initial_image = \"\" #@param {type:\"string\"}\n",
        "checkForInitial_Image = len(initial_image)\n",
        "if checkForInitial_Image == 0:\n",
        "  init_image_or_folder = defaultInitPath\n",
        "else:\n",
        "  init_image_or_folder = defaultInitPath + initial_image\n",
        "init_strength = 0 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##6---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 6): Sequence Settings - Cycles** *(Through Ranges - optional):* <br></u>\n",
        "#<br>\n",
        "#@markdown `Note: use cycleThroughPromptList to create variations with the prompt templates`\n",
        "# for template cycling, ex generating both male and female or sketch after the other:\n",
        "cycleThroughPromptList = False#@param{type:\"boolean\"}\n",
        "templatesToCycle = \"1,2\" #@param {type:\"string\"}\n",
        "listOfTemplatesToCycle = templatesToCycle.split(\",\")\n",
        "numOfTemplatesToCycle = len(listOfTemplatesToCycle)\n",
        "# end template cycle variables-----\n",
        "# these options would run 14 times:\n",
        "init_strength_0_to_1 = False#@param{type:\"boolean\"}\n",
        "cfg_scale_7_to_20 = False#@param{type:\"boolean\"}\n",
        "init_strength_random = False#@param{type:\"boolean\"}\n",
        "cfg_scale_random = False#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##7---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 7): Sequence Settings with Images** *(optional interpolation):* <br></u>\n",
        "#<br>\n",
        "# forgot what the next one does, it wasn't as good as doing the image cycle though:\n",
        "useGeneratedImageAsInit = False#param{type:\"boolean\"}\n",
        "# for interpolating using generated image cycles on INIT_STRENGTH:\n",
        "generatedImageCycle = False#@param{type:\"boolean\"} \n",
        "specifyStrengthLock = -1 #@param {type:\"number\"} # -1 is default to go from 0.6 to 0.2; or you can lock to only gen say 0.05. It doesn't blend as well locking to a single value\n",
        "activateStrengthLock = False\n",
        "if specifyStrengthLock == -1:\n",
        "  activateStrengthLock = False\n",
        "else:\n",
        "  activateStrengthLock = True\n",
        "# prompts over time lets us pick keyframe for duration of certain prompts\n",
        "usePromptsOverTimeCycle = False#@param{type:\"boolean\"}\n",
        "# sometimes there's too large a jump when we go down to 0.2 inti strngth so this has smoother results:\n",
        "promptOverTimeSetting03InsteadOf02 = False#@param{type:\"boolean\"}\n",
        "promptsOverTimeFramesList = []\n",
        "promptsOverTimePromptList = []\n",
        "totalFrames = 0\n",
        "# if we're using prompts over time, we take the user created DICT of frame count / prompts and split to new list\n",
        "# based on frame count, we'll duplicate that prompt into the prompt list the rest of the script will read\n",
        "if usePromptsOverTimeCycle == True:\n",
        "  firstPromptOnly = False\n",
        "  prompts.clear()\n",
        "  promptsA.clear()\n",
        "  promptsB = \"\"\n",
        "  for key, values in promptInterpolateMulti.items():\n",
        "     number = int(values[0]) # frame count\n",
        "     promptsOverTimeFramesList.append(number) # add to list of frame counts, ex. 6\n",
        "     promptsOverTimePromptList.append(values[1]) # adds to list of prompt strings, ex. 'cat'\n",
        "  for x in promptsOverTimeFramesList:\n",
        "    totalFrames += int(x) # this value will be the total of the prompts / frames we gen\n",
        "  number_of_repeats = totalFrames # we might as well pick an accurate generation count to match our frame count. so override this user inputted variable\n",
        "  counta = 0\n",
        "  zz = 0\n",
        "  for y in range(totalFrames):\n",
        "    promptToSend = promptsOverTimePromptList[zz]\n",
        "    promptsA.append(promptToSend)\n",
        "    counta += 1\n",
        "    if counta == (promptsOverTimeFramesList[zz]):\n",
        "      zz += 1\n",
        "      counta = 0\n",
        "  if templateSelectedKey == 0:\n",
        "    promptsB = [x + \", \" + midDetails for x in promptsA] # add the universal style text to each prompt in the list\n",
        "    prompts = [x + \", \" + endingStyle for x in promptsB] # add the universal style text to each prompt in the list\n",
        "  else:\n",
        "    prompts = [x + \", \" + templateSelected for x in promptsA] # add the universal style text to each prompt in the list\n",
        "# ----- end set up of the usePromptsOverTimeCycle / multi prop with frame count cycle interpolation setup -----------------\n",
        "\n",
        "save_settings_file = True#param{type:\"boolean\"} # it seems to break script if this is set to False, so I hid it\n",
        "\n",
        "# Sequence Counter for evaluating cfg and init_strength scales:\n",
        "areBothScalesToGen = False\n",
        "if init_strength_0_to_1 == True:\n",
        "  firstPromptOnly = True\n",
        "  if cfg_scale_7_to_20 == True:\n",
        "    number_of_repeats = 80 # value if both options are selected\n",
        "    areBothScalesToGen = True\n",
        "  else:\n",
        "    number_of_repeats = 10\n",
        "if cfg_scale_7_to_20 == True:\n",
        "  firstPromptOnly = True\n",
        "  if init_strength_0_to_1 == True:\n",
        "    number_of_repeats = 80 # value if both options are selected\n",
        "    areBothScalesToGen = True\n",
        "  else:\n",
        "    number_of_repeats = 8\n",
        "counterCFG = 6\n",
        "counterINITSTRENGTH = 0\n",
        "counter = 0\n",
        "countRepeats = -1\n",
        "number_of_repeats_display = number_of_repeats\n",
        "randomNumber = 0\n",
        "countPromptTemplateCycles = -1\n",
        "countSubfolderNumber = 0\n",
        "sample_idx = 1 # reset this number every generation\n",
        "\n",
        "# INIT_Image gen from drive:\n",
        "activateUsePreviousGenImage = False # we have to activate this later in the code\n",
        "areWeOnOddInitGenCycle = True # odd will gen random seed at 0.3, even will fen previous seed at 0.7\n",
        "seedA_GenCycle = random.randint(0, 2**32) if seed == -1 else seed\n",
        "seedB_GenCycle = random.randint(0, 2**32)\n",
        "reset_init_strength_for_gen_cycle = False\n",
        "if generatedImageCycle == True:\n",
        "  init_strength = 0.9 #starting value for cycle; we go from 0.6 down to 0.2 in 0.1 increments\n",
        "  init_strength_0_to_1 = False\n",
        "  cfg_scale_7_to_20 = False\n",
        "\n",
        "if firstPromptOnly == True:\n",
        "  number_of_repeats_display = number_of_repeats\n",
        "else:\n",
        "  number_of_repeats_display = number_of_repeats * len(prompts)\n",
        "\n",
        "# check if we're cycling through prompt templates:\n",
        "if cycleThroughPromptList == True:\n",
        "  if init_strength_0_to_1 == False and cfg_scale_7_to_20 == False:\n",
        "    if usePromptsOverTimeCycle == False:\n",
        "      number_of_repeats = numOfTemplatesToCycle * len(prompts)\n",
        "      number_of_repeats_display = number_of_repeats\n",
        "    elif usePromptsOverTimeCycle == True:\n",
        "      number_of_repeats = numOfTemplatesToCycle\n",
        "      number_of_repeats_display = number_of_repeats\n",
        "\n",
        "if usePromptsOverTimeCycle == True:\n",
        "  number_of_repeats_display = number_of_repeats\n",
        "  number_of_repeats = 1 # we only repeat once since we already compiled all our prompts\n",
        "\n",
        "# --------- CODE STARTS GENERATING ----------------\n",
        "for x in range(number_of_repeats):\n",
        "  countRepeats +=1\n",
        "  countSubfolderNumber = 0\n",
        "  # These counters count every time the prompt is sent for another repeat\n",
        "  if firstPromptOnly == True:\n",
        "    if init_strength_0_to_1 == False or cfg_scale_7_to_20 == False:\n",
        "      counter = counter\n",
        "  else:\n",
        "    if init_strength_0_to_1 == False:\n",
        "      counter = 0\n",
        "    if cfg_scale_7_to_20 == False:\n",
        "      counter = 0\n",
        "  if init_strength_0_to_1 == True or cfg_scale_7_to_20 == True:\n",
        "    if countRepeats == number_of_repeats:\n",
        "      break\n",
        "\n",
        "  # check if we're cycling through prompt templates and need to swap the prompt text:\n",
        "  if cycleThroughPromptList == True:\n",
        "    countPromptTemplateCycles += 1\n",
        "    if templateSelectedKey == number_of_repeats: # prevents the list going out of range\n",
        "      break\n",
        "    try:\n",
        "      templateSelectedKey = listOfTemplatesToCycle[countPromptTemplateCycles] # for some reason this line turns the into to string, so have to turn it back to int\n",
        "    except:\n",
        "      break\n",
        "    promptTemplatedSelectedlist = promptTemplateDict[int(templateSelectedKey)] # gets both the values of the selected key as a list, ex ['male', 'line art']\n",
        "    templateSelectedStart = promptTemplatedSelectedlist[0]\n",
        "    templateSelected = promptTemplatedSelectedlist[1]\n",
        "    prompts = [x + \", \" + templateSelected for x in promptsA] # add the universal style text to each prompt in the list\n",
        "\n",
        "\n",
        "  # calculating the full prompt: ----------------------------------\n",
        "  for prompt in prompts:\n",
        "    # check if we are only supposed to generate the first prompt or all them:\n",
        "    if firstPromptOnly == True:\n",
        "      if prompt == prompts[1]:\n",
        "        break\n",
        "\n",
        "    # the random start and end words are broken, for some reason\n",
        "    if numOfRandomStringStart != 0 and numOfRandomStringStart != 1:\n",
        "      # wordRNDM = random.choice(randomStringLS) # trash line didn't run properly\n",
        "      number = ((seed - seed) + (numOfRandomStringStart - 1))\n",
        "      randomNumber = random.randint(0, number)\n",
        "      # print(numOfRandomStringStart)\n",
        "      if randomNumber > numOfRandomStringStart or randomNumber <0:\n",
        "        randomNumber = 0\n",
        "      wordRNDM = randomStringLS[randomNumber]\n",
        "    elif numOfRandomStringStart == 1:\n",
        "      wordRNDM = randomStringLS[0]\n",
        "    #print(randomNumber)\n",
        "    #print(wordRNDM)\n",
        "\n",
        "    if numOfRandomStringEnd != 0 and numOfRandomStringEnd != 1:\n",
        "      # wordRNDMend = random.choice(randomStringEnd) # this trash didn't work for some reason\n",
        "      number = ((seed - seed) + (numOfRandomStringEnd - 1))\n",
        "      randomNumber = random.randint(0, number)\n",
        "      if randomNumber > numOfRandomStringEnd or randomNumber <0:\n",
        "        randomNumber = 0\n",
        "      wordRNDMend = randomStringEnd[randomNumber]\n",
        "    elif numOfRandomStringEnd == 1:\n",
        "      wordRNDMend = randomStringEnd[0]\n",
        "\n",
        "    if templateSelected != \"none\":\n",
        "      randomStringLS = templateSelectedStart\n",
        "      wordRNDM = templateSelectedStart\n",
        "\n",
        "    # opt.prompt = prompt\n",
        "    if numOfRandomStringStart == 0:\n",
        "      if numOfRandomStringEnd == 0:\n",
        "        promptPreFinal = prompt\n",
        "      else:\n",
        "        if len(wordRNDMend) == 0:\n",
        "          promptPreFinal = prompt\n",
        "        else:\n",
        "          promptPreFinal = prompt + \",\" + wordRNDMend\n",
        "    else:\n",
        "      if numOfRandomStringEnd == 0:\n",
        "        promptPreFinal = wordRNDM + \", \" + prompt\n",
        "      else:\n",
        "        if len(wordRNDMend) == 0:\n",
        "          promptPreFinal = wordRNDM + \", \" + prompt\n",
        "        else:\n",
        "          promptPreFinal = wordRNDM + \", \" + prompt + \",\" + wordRNDMend\n",
        "\n",
        "    # finally bring the combined prompt together:\n",
        "    if emoteTemplateSelected == \"none\":\n",
        "      opt.prompt = promptPreFinal\n",
        "    else:\n",
        "      opt.prompt = promptPreFinal + \" \" + emoteTemplateSelected\n",
        "\n",
        "    shortenedPromptNameList = prompt.split()[:3]\n",
        "    shortenedPromptNameStr = \" \".join(shortenedPromptNameList)\n",
        "    batch_name = clean_string(shortenedPromptNameStr) # shorten the prompt word since the prompts are too long for windows!\n",
        "    if generatedImageCycle == True:\n",
        "      amountOfPaddingForN = len(str(number_of_repeats)) # how much padding to get\n",
        "      xNum = f'{sample_idx:{0}{amountOfPaddingForN}}'\n",
        "      paddedBatchName = str(xNum) + \" \" + batch_name\n",
        "      batch_name = paddedBatchName\n",
        "\n",
        "    # Related to INIT IMAGES:\n",
        "    if init_imgs_list[0] != \"none\":\n",
        "      # opt.outdir = os.path.join(outputs_path, batch_name) # if you want output dir to match the prompt\n",
        "      if randomizeImageFromFolder == True and useGeneratedImageAsInit == False and generatedImageCycle == False:\n",
        "        opt.init_img = init_image_or_folder\n",
        "      if randomizeImageFromFolder == False and generatedImageCycle == False:\n",
        "        if checkForInitial_Image == 0: # we have to check if user wants to run a single specific image (meaning the field in the init image area)\n",
        "          if firstPromptOnly == False:\n",
        "            if init_strength == 0:\n",
        "              print(\"\")\n",
        "              # full_init_img_path = init_image_or_folder + init_imgs_list[counter]\n",
        "            else:\n",
        "              full_init_img_path = init_image_or_folder + init_imgs_list[counter]\n",
        "          else:\n",
        "            full_init_img_path = init_image_or_folder + init_imgs_list[0]\n",
        "        else:\n",
        "          full_init_img_path = init_image_or_folder\n",
        "        print(\"Init image used: \" + full_init_img_path)\n",
        "        opt.init_img = full_init_img_path\n",
        "\n",
        "\n",
        "    opt.ddim_steps = steps\n",
        "    opt.n_iter = 1\n",
        "    opt.n_samples = samples_per_batch\n",
        "\n",
        "    # seed creation and the cycle init image gen from itself:\n",
        "    if generatedImageCycle == False:\n",
        "      # pick random seed for images or the seed specified:\n",
        "      opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "    else:\n",
        "      # this only activates if we are trying to cycle through INIT image and reuse it from the generated location:\n",
        "      # first gen only:\n",
        "      if activateUsePreviousGenImage == False:\n",
        "        opt.init_img = init_image_or_folder # do only on the first generation to get our input image\n",
        "        activateUsePreviousGenImage = True\n",
        "\n",
        "      # core cycle:\n",
        "      init_strength_endPoint = 0.2 # how far we go from 0.6 to 0.2 or 0.3\n",
        "      if promptOverTimeSetting03InsteadOf02 == True:\n",
        "          init_strength_endPoint = 0.3 # for a more subtle interpolation\n",
        "      if areWeOnOddInitGenCycle == True:\n",
        "        opt.seed = seedA_GenCycle\n",
        "        if activateStrengthLock == False:\n",
        "          init_strength -= 0.1 # only go down 0.1 if there is no strength lock\n",
        "          round(init_strength)\n",
        "          if init_strength == init_strength_endPoint or init_strength <= (init_strength_endPoint + 0.1): # when we get to the end; either 0.2 or 0.3\n",
        "            areWeOnOddInitGenCycle = False\n",
        "            reset_init_strength_for_gen_cycle = True\n",
        "            seedA_GenCycle +=1\n",
        "        elif activateStrengthLock == True:\n",
        "          areWeOnOddInitGenCycle = False\n",
        "          init_strength = specifyStrengthLock\n",
        "          reset_init_strength_for_gen_cycle = True\n",
        "          seedA_GenCycle +=1\n",
        "\n",
        "      elif areWeOnOddInitGenCycle == False:\n",
        "        # opt.seed is previous result\n",
        "        opt.seed = seedB_GenCycle\n",
        "        if activateStrengthLock == False:\n",
        "          init_strength -= 0.1\n",
        "          round(init_strength)\n",
        "          if init_strength == init_strength_endPoint or (init_strength_endPoint + 0.1):\n",
        "            areWeOnOddInitGenCycle = True\n",
        "            reset_init_strength_for_gen_cycle = True\n",
        "            seedB_GenCycle +=1\n",
        "        elif activateStrengthLock == True:\n",
        "            areWeOnOddInitGenCycle = True\n",
        "            init_strength = specifyStrengthLock\n",
        "            reset_init_strength_for_gen_cycle = True\n",
        "            seedB_GenCycle +=1     \n",
        "          # ---------- END THE CYCLE THROUGH INIT --------------------------\n",
        "      \n",
        "    # choose to pull one of the generated images as INIT Image:\n",
        "    if useGeneratedImageAsInit == True and generatedImageCycle == False:\n",
        "      if activateUsePreviousGenImage == False:\n",
        "        activateUsePreviousGenImage = True\n",
        "      elif activateUsePreviousGenImage == True:\n",
        "        # this will only run when activateUsePreviousGenImage = True as set above\n",
        "        if generatedImageCycle == False:\n",
        "          while not os.path.exists(lastGeneratedImagePath):\n",
        "            time.sleep(1) \n",
        "          if os.path.isfile(lastGeneratedImagePath):\n",
        "            opt.init_img = lastGeneratedImagePath\n",
        "            seed += 1 # this technique only works if you use different seeds\n",
        "\n",
        "    # CFG Calculation:    \n",
        "    opt.sampler = sampler\n",
        "    if cfg_scale_7_to_20 == False:\n",
        "      opt.scale = guidance_scale\n",
        "    else:\n",
        "      opt.scale = counterCFG\n",
        "    if cfg_scale_random == True:\n",
        "      opt.scale = random.uniform(7, 20)\n",
        "\n",
        "    # INIT IMAGE:\n",
        "    if init_strength_0_to_1 == True:\n",
        "      init_strength = counterINITSTRENGTH\n",
        "    if init_strength_random == True:\n",
        "      init_strength = random.uniform(0.1, 0.8)\n",
        "    opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "\n",
        "    #opt.W = width_height[0]\n",
        "    #opt.H = width_height[1]\n",
        "    opt.W, opt.H = map(lambda x: x - x % 64, (width_height[0], width_height[1])) # resize to integer multiple of 64\n",
        "\n",
        "    # Counters:\n",
        "    counter += 1 # I added this to keep track of INIT images\n",
        "    if firstPromptOnly == True:\n",
        "      if cycleThroughPromptList == True or usePromptsOverTimeCycle == True:\n",
        "        print(\"Generations processed: \" + str(counter) + \" / \" + str(numOfTemplatesToCycle))\n",
        "      else: \n",
        "        print(\"Generation processing: \" + str(counter) + \" / \" + str(number_of_repeats_display))    \n",
        "    else:\n",
        "        displayCounter = (len(prompts) * countRepeats) + counter\n",
        "        if cycleThroughPromptList == True:\n",
        "          print(\"Generations processed: \" + str(counter) + \" / \" + str(numOfTemplatesToCycle * len(prompts)))\n",
        "        else:        \n",
        "          print(\"Generation processing: \" + str(displayCounter) + \" / \" + str(number_of_repeats_display))\n",
        "    if areBothScalesToGen == False:\n",
        "      counterCFG += 2 # go up from 7 to 20 if activated\n",
        "      counterINITSTRENGTH = counterINITSTRENGTH + 0.1 # go up from 0.1 to 0.8 if activated\n",
        "    else:\n",
        "      counterINITSTRENGTH = counterINITSTRENGTH + 0.1\n",
        "      if counterINITSTRENGTH > 0.9:\n",
        "        counterINITSTRENGTH = 0 # reset since we're now going to go to next CFG\n",
        "        counterCFG += 2 # go up from 7 to 20 if activated\n",
        "\n",
        "    if opt.strength >= 1 or init_image_or_folder == None or opt.strength <= 0:\n",
        "        opt.init_img = \"\"\n",
        "\n",
        "    if opt.init_img != None and opt.init_img != '':\n",
        "        opt.sampler = 'ddim'\n",
        "\n",
        "    if opt.sampler != 'ddim':\n",
        "        opt.ddim_eta = 0.0\n",
        "\n",
        "    # debugging during processing:\n",
        "    print(\"prompt: \" + opt.prompt[:110] +\"...\")\n",
        "\n",
        "    # save settings\n",
        "    settings = {\n",
        "        'ddim_eta': ddim_eta,\n",
        "        'guidance_scale': guidance_scale,\n",
        "        'init_image': init_image_or_folder,\n",
        "        'init_strength': init_strength,\n",
        "        'number_of_images': number_of_images,\n",
        "        'prompt': opt.prompt,\n",
        "        'sampler': sampler,\n",
        "        'samples_per_batch': samples_per_batch,\n",
        "        'seed': opt.seed,\n",
        "        'steps': steps,\n",
        "        'width': opt.W,\n",
        "        'height': opt.H,\n",
        "    }\n",
        "    # saving and making output directories:\n",
        "    countSubfolderNumber += 1\n",
        "    if saveSeperatePromptsToSubfolders == False:\n",
        "      opt.outdir = os.path.join(outputs_path, outputFolder)\n",
        "    else:\n",
        "      numToUsePath = countSubfolderNumber.zfill(3)\n",
        "      opt.outdir = os.path.join(outputs_path, outputFolder + \"/\" + str(numToUsePath) + \" \" + shortenedPromptNameStr)\n",
        "\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    # filePathToSaveTo = opt.outdir + \"/\" + batch_name + \"\" + batch_idx.zfill(3)  + \"_\" + sample_idx.zfill(4) + \" \" + {opt.seed} + \".png\"\n",
        "    pathBatchIDPad = (f'{batch_idx:03}')\n",
        "    pathsample_idx = (f'{sample_idx:04}') \n",
        "    if numberBeginningOfFile == False:\n",
        "      lastGeneratedImagePath = opt.outdir + \"/\" + batch_name + \"(\" + str(pathBatchIDPad)  + \")_\" + str(pathsample_idx) + \" \" + str(opt.seed) + \".png\"\n",
        "    else:\n",
        "      lastGeneratedImagePath = opt.outdir + \"/\" + str(pathsample_idx) + \" \" + batch_name + \"(\" + str(pathBatchIDPad)  + \")\" + \" \" + str(opt.seed) + \".png\"\n",
        "    # while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_{sample_idx:04}_settings.txt\"):\n",
        "        #countSubfolderNumber += 1\n",
        "        # print( \"counting the prompt: \" + str(countSubfolderNumber))\n",
        "    if save_settings_file:\n",
        "      if numberBeginningOfFile == False:\n",
        "        with open(f\"{opt.outdir}/{batch_name}({batch_idx})_{sample_idx:04}_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "            json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "      else:\n",
        "        with open(f\"{opt.outdir}/{sample_idx:04} {batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "            json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "    with open(f\"{opt.outdir}/prompt.txt\", 'w') as f:\n",
        "          f.write(opt.prompt)\n",
        "    # sample_idx = 0\n",
        "\n",
        "    for i in range(number_of_images):\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        images = generate(opt)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Used seed: {opt.seed}\")\n",
        "        if firstPromptOnly == True:\n",
        "          if cycleThroughPromptList == True or usePromptsOverTimeCycle == True:\n",
        "            print(\"Generations processed: \" + str(counter) + \" / \" + str(numOfTemplatesToCycle))\n",
        "          else:\n",
        "            print(\"Generations processed: \" + str(counter) + \" / \" + str(number_of_repeats_display))\n",
        "        else:\n",
        "          if cycleThroughPromptList == True:\n",
        "            print(\"Generations processed: \" + str(counter) + \" / \" + str(numOfTemplatesToCycle * len(prompts)))\n",
        "          else:\n",
        "            print(\"Generations processed: \" + str(displayCounter) + \" / \" + str(number_of_repeats_display))\n",
        "        # print(f\"Saved to: {opt.outdir}\")\n",
        "        print(\"prompt: \" + opt.prompt[:110] +\"...\")\n",
        "        # print(lastGeneratedImagePath)\n",
        "        # print (\"countdown of generatedImageCycle: \" + str(init_strength))\n",
        "\n",
        "        #for image in images:\n",
        "        #    display(image)\n",
        "        display(images[0])\n",
        "\n",
        "        batch_idx += 1\n",
        "        sample_idx += 1\n",
        "        # opt.seed += 1\n",
        "\n",
        "    # change some settings like swap to previous generated image to cycle though generatedImageCycle:  \n",
        "    if generatedImageCycle == True:\n",
        "      if reset_init_strength_for_gen_cycle == True:\n",
        "        reset_init_strength_for_gen_cycle = False\n",
        "        init_strength = 0.7\n",
        "        print(\"wait while loading... \" + lastGeneratedImagePath)\n",
        "        while not os.path.exists(lastGeneratedImagePath):\n",
        "          time.sleep(1) \n",
        "        if os.path.isfile(lastGeneratedImagePath):\n",
        "          opt.init_img = lastGeneratedImagePath\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Prompt Cells (run)\n",
        "- (run once at start and whenever you edit these)"
      ],
      "metadata": {
        "id": "B8rTt1lqb3uy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SS-UZf77N7x",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # CELL 1: PROMPT TEMPLATE (run)\n",
        "Info = 'click Show Code to view, click Run Cell to update:' #@param [\"click Show Code to view, click Run Cell to update:\"]\n",
        "\n",
        "# Here you can save prompt templates for reusing:\n",
        "# if you want to edit or add your own to the template, make sure to respect Python dict: key: ['templateStartWord', 'prompt details'],\n",
        "# in addition to writing the prompts here, you need to add them to the promptTemplate #@param in the main code, otherwise it won't show up to select\n",
        "\n",
        "promptTemplateDict = {\n",
        "0: ['', 'none'],\n",
        "1: ['female', 'portrait, young woman, detailed gorgeous face, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by artgerm and greg rutkowski and alphonse mucha'],\n",
        "2: ['male', 'portrait, mature man, detailed rugged face, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by J. C. Leyendecker and greg rutkowski and Norman Rockwell'],\n",
        "3: ['science fiction', 'scifi, robotics, fururistic decor, trending on artstation, global illumination, matte painting, high detail, unreal engine 5, halo, star citizen, star wars, by sparth, octane render, by ralph McQuarrie, by Syd Mead, by ryan church'],\n",
        "4: ['fantasy', 'painterly, nature, natural, guild wars, magical, ethereal, highly detailed, historical, period piece, by greg rutkowski, by Daniel Dociu, by noah bradley, by howard lyon'],\n",
        "5: ['lineart drawing', 'drawn by bernie wrightson, industrial design sketch, sketch by Scott Robertson'],\n",
        "6: ['Redshift render', 'sharp, rendered in octane, highly detailed, minimalistic, rendered in unreal engine 5, product render'],\n",
        "7: ['hard surface, weapon design, scifi gun, halo, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by greg rutkowski'],\n",
        "8: ['a wholesome animation key shot of', 'studio ghibli, pixar and disney animation, sharp, rendered in unreal engine 5, anime key art by greg rutkowski, bloom, dramatic lighting'],\n",
        "9: ['a scene of', 'black and white, comic book art, by yoji shinkawa and takehiko inoue and kim jung gi, masterpiece, perfect'],\n",
        "10: ['intricate fine tipped pen drawing of a', 'inktober, Fine Line Tattoo, manga line art, monochrome, dotwork, by dan hilliard, by Stanislaw Wilczynski, by alphonse mucha, by aaron horkey'],\n",
        "11: ['full body 3d render of', 'as a vinyl action figure, studio lighting, white background, blender, trending on artstation, 8k, highly detailed, rendered in redshift, studio product photograph'],\n",
        "12: ['sticker of', 'cute sticker decal design, highly detailed, high quality, digital painting'],\n",
        "13: ['marble statue', 'sculpture, marble, ancient greek statue, museum statue from stone, stone sculpt'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #CELL 2a: INTERPOLATION PROMPTS (run this cell & the next one)\n",
        "Info = 'click Show Code to view, click Run Cell to update:' #@param [\"click Show Code to view, click Run Cell to update:\"]\n",
        "\n",
        "# use this section to change the prompt being interpolated over time (frames held by prompt)\n",
        "\n",
        "# More Information:\n",
        "# key#: [frameCount, promptString]\n",
        "# at 30% playback speed of 30FPS video, #10 frames = 1 second\n",
        "# best frame length 0.2: initial prompt = chaneg on 8 or 13 (5 increments); after change on 5,10,15, etc (this will change prompt as soon as we land back on 0.6, granted we go down to 0.2)\n",
        "# best frame length 0.3: initial prompt = change on 7 or 11 (4 increments); after change on 4,8,12, etc\n",
        "\n",
        "promptInterpolateMulti = {\n",
        "0: [10, 'a frog relaxing on a lilypad while ribbiting'],\n",
        "1: [10, 'a frog transforming into a cat, white background'],\n",
        "2: [10, 'a cat hissing and growling in a creepy forest, white background'],\n",
        "3: [10, 'a cat growing into a massive lion, white background'],\n",
        "4: [10, 'a lion roaring, white background'],\n",
        "5: [20, 'a exploding, explosion, burst of flames'],\n",
        "6: [10, 'a face of a beautiful woman turning into a flower'],\n",
        "7: [30, 'flower pedals transforming into a swarm of butterflies'],\n",
        "8: [20, 'flower pedals transforming into a swarm of butterflies'],\n",
        "9: [20, 'butterflies shrinking into tiny stars in a starry night'],\n",
        "}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FcYglLg7ra0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Tools *(stand alone by section header)*\n",
        "- | Upscaling (HD+ Tiles) | Facial Recognition | Upscaling (Real ESRGAN) |"
      ],
      "metadata": {
        "id": "lx5EjEc5bgRV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzsZgcm37USQ"
      },
      "source": [
        "## <u> A) Post Processing - **Grid Upscaling** </u> *(Tile Generation)* "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE BLOCK 1) Split an image into grids of 512x512 size\n",
        "#@title 1) Upscaling and HD Resolution Tile Generator:\n",
        "#@markdown `Note: this script requires a final image as input. It'll detail 512x512 grids across the input image, so your image should be sized up already to 2048+`<br>\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "from itertools import product\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "import math\n",
        "import glob\n",
        "from google.colab import output\n",
        "import shutil\n",
        "\n",
        "inputImage = \"landscape2.png\" #@param {type:\"string\"}\n",
        "inputDirectory = \"/content/init_image\" #@param {type:\"string\"}\n",
        "outputDirectory = \"/content/HD/preProcessedGridSplits\"\n",
        "outputDirectoryFiles = glob.glob('/content/HD/preProcessedGridSplits/**/*', recursive=True)\n",
        "#saveSeperatePromptsToSubfolders = False#@param{type:\"boolean\"}\n",
        "\n",
        "def splitImageTiles(filename, dir_in): # (inputImage, inputDirectory)\n",
        "    # setup:\n",
        "    d = 512 # the width / height we'll split our image into; it's 512 since this is the ideal size to input anyways & the math ONLY works with 512x512 size\n",
        "    # dir_out = dir_in + \"\\\\\" + \"outSplitGrids\"\n",
        "    dir_out = outputDirectory\n",
        "\n",
        "    # clean up existing files in the output Directory:\n",
        "    for f in outputDirectoryFiles:\n",
        "        try:\n",
        "            os.remove(f)\n",
        "        except OSError as e:\n",
        "            print(\"Error: %s : %s\" % (f, e.strerror))\n",
        "\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    img = Image.open(os.path.join(dir_in, filename))\n",
        "    w, h = img.size # image size, ex (2048, 3072)\n",
        "    #print(w)\n",
        "\n",
        "    # modify the width of the image to work well with the code as a power of 256:\n",
        "    ratioSize = d/2 # the ratio we want to match is half of our image width\n",
        "    nearest_multipleOf256 = int(ratioSize * round(w/ratioSize))\n",
        "    wpercent = (nearest_multipleOf256 / float(img.size[0]))\n",
        "    hsize = int((float(img.size[1]) * float(wpercent)))\n",
        "    img = img.resize((nearest_multipleOf256, hsize), Image.ANTIALIAS) # resize the image so that the width at least matches clean to our grid\n",
        "    # img.save(dir_in + 'resized_image.png')\n",
        "    w, h = img.size # image size, ex (2048, 3072)\n",
        "    outputGridImagesList = []\n",
        "\n",
        "    ratioW = w / d\n",
        "    ratioH = h / d\n",
        "    ratioHInitial = ratioH\n",
        "\n",
        "    ratioW = int(ratioW) # for example 4\n",
        "    ratioH = int(ratioH) # for example 6\n",
        "    # for some reason int rounds them all down, so we have to accomodate for these ratio possibilities\n",
        "    # 1) images can be less than 0.05 difference, 2) images can be between 0.1 and 0.5 difference and 3) images can be greater than (0.5-0.99) difference\n",
        "    cutsW = ratioW*2 # the amount of cut images on the wdith; the columns ; for example 8\n",
        "    if ratioHInitial - ratioH < 0.05:\n",
        "        cutsH = ratioH*2 # the amount of cut images on the height; the rows ; for example 12\n",
        "    elif ratioHInitial - ratioH >= 0.5:\n",
        "        cutsH = int((ratioH*2) + 2) # the amount of cut images on the height; the rows ; for example 12\n",
        "    else:\n",
        "        cutsH = int((ratioH*2) + 1)\n",
        "    countW = 0\n",
        "    countH = 1 # we start with 1 since we will start in the first row and we're counting by width / countW\n",
        "    image_counter = 0\n",
        "\n",
        "    # before processing the images, check if a folder to store the new images exists:\n",
        "    isExist = os.path.exists(dir_out)\n",
        "    if not isExist:\n",
        "        os.makedirs(dir_out)    \n",
        "\n",
        "    # grid = product(range(0, h-h%d, int(d/2)), range(0, w-w%d, int(d/2)))\n",
        "    grid = product(range(0, h*2-h%d, int(d/2)), range(0, w-w%d, int(d/2)))\n",
        "    for i, j in grid:\n",
        "        countW += 1\n",
        "        #print(countW)\n",
        "        box = (j, i, j+d, i+d)\n",
        "        # out = os.path.join(dir_out, f'{name}_{i}_{j}{ext}')\n",
        "        # out = os.path.join(dir_out, f'{name}_{countW}_{countH}{ext}')\n",
        "        out = os.path.join(dir_out, f'preGrid_{str(countH).zfill(3)}_{str(countW).zfill(3)}{ext}')\n",
        "        img.crop(box).save(out)\n",
        "        outputGridImagesList.append(out)\n",
        "\n",
        "        if countW == cutsW:\n",
        "            countW = 0\n",
        "            countH += 1\n",
        "\n",
        "        if countH == (cutsH + 1):\n",
        "            output.clear()\n",
        "            image_counter +=1\n",
        "            print(\"Split: \" + str(image_counter) + \" pieces split\")\n",
        "            print(\"Finished processing! Now write descriptions for each section:\")\n",
        "            break\n",
        "      \n",
        "        output.clear()\n",
        "        image_counter +=1\n",
        "        print(\"Split: \" + str(image_counter))\n",
        "    \n",
        "    return w, h, image_counter, cutsW, outputGridImagesList\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Core Code:\n",
        "# 1) Split the images into seperate grids which we'll feed into Stable Diffusion:\n",
        "widthAndHeight = splitImageTiles(inputImage,inputDirectory) # run the code\n",
        "initialImageWidth = (widthAndHeight[0])\n",
        "initialImageHeight = (widthAndHeight[1]) # we especially need this for later\n",
        "totalSplitCount = (widthAndHeight[2]) # to determine if we have the correct # of images\n",
        "numberOfImageAlongX = (widthAndHeight[3]) # check how many split cells go along the X axis, to check when we reach the end later\n",
        "listOfOutputImages = (widthAndHeight[4]) # the list of the output images by their path, ex: '/content/preProcessedGridSplits/preGrid_001_001.png'\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 2) Get the user to input their desired text to describe each image block:\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import time\n",
        "\n",
        "imageDescriptionsList = []\n",
        "img = Image.open(os.path.join(listOfOutputImages[0]))\n",
        "time.sleep(1)\n",
        "\n",
        "if os.path.exists(\"/content/HD/text/\") == False:\n",
        "  os.makedirs(\"/content/HD/text/\")\n",
        "# note: the following will wipe out the input textfile every time we run code:\n",
        "with open('/content/HD/text/descriptionsInput.txt', 'w') as f:\n",
        "  f.write('')\n",
        "filePathTextToUse = '/content/HD/text/descriptionsInput.txt'\n",
        "\n",
        "for num in range((totalSplitCount - 1)):\n",
        "  print (\"Describing: \" + str(num+1) + \" / \" + str (totalSplitCount))\n",
        "  print (\"NOTE: If you wish to use a txt file instead of typing here, type EXIT\")\n",
        "  display(img)\n",
        "  time.sleep(0.3)\n",
        "  imageDescription = input('Enter a description for this image:\\n')\n",
        "  if imageDescription == \"EXIT\":\n",
        "    clear_output()\n",
        "    print (\"Cancelled typing prompt descriptions\")\n",
        "    time.sleep(0.1)\n",
        "    differentTextFileToUse = input('Type the full path of the text file you wish to use:\\n')\n",
        "    filePathTextToUse = differentTextFileToUse\n",
        "    break\n",
        "  else:\n",
        "    imageDescriptionsList.append(imageDescription)\n",
        "    with open('/content/HD/text/descriptionsInput.txt', 'a') as f: \n",
        "      f.write(imageDescription)\n",
        "      f.write('\\n')\n",
        "    clear_output()\n",
        "    nextImage = num+1\n",
        "    img = Image.open(os.path.join(listOfOutputImages[nextImage]))\n",
        "    time.sleep(0.1)\n",
        "\n",
        "    with open('/content/HD/text/descriptionsInput.txt', 'a') as f: \n",
        "      f.write(imageDescription)\n",
        "\n",
        "shutil.copy('/content/HD/text/descriptionsInput.txt', '/content/HD/text/descriptionsInput_AUTOBACKUP.txt') # Use the shutil module\n",
        "print(\"done\")\n",
        "\n",
        "# now we read the file we want to use as the text prompter and turn it into a list:\n",
        "# fileText = open(filePathTextToUse, 'r')\n",
        "with open(filePathTextToUse) as f:\n",
        "    Lines = [Line.rstrip() for Line in f]\n",
        "promptDescriptionsToAppend = Lines # here's the list of lines we'll use to go with the images\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 3) Now we can run these prompts and images through Stable Diffusion to Upres them!\n",
        "\n",
        "\n",
        "#@markdown Basic settings:\n",
        "batch_name = \"testsB\" #@param {type:\"string\"}\n",
        "gridRatio = \"672\" #@param [512, 608, 640, 672, 704]\n",
        "width_height = [int(gridRatio), int(gridRatio)] #param{type: 'raw'}\n",
        "guidance_scale = 25 #@param {type:\"number\"}\n",
        "steps = 40 #@param {type:\"integer\"}\n",
        "init_strength = 0.8 #@param {type:\"number\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "num_batch_images = 1 #param {type:\"integer\"}\n",
        "sampler = 'ddim' #@param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 #param {type:\"number\"}\n",
        "seed = 0 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Prompt\n",
        "promptStart = \"forest environment design\" #@param {type:\"string\"}\n",
        "promptEnd = \"painterly, nature, natural, guild wars, magical, ethereal, highly detailed, historical, period piece, by greg rutkowski, by Daniel Dociu, by noah bradley, by howard lyon\" #@param {type:\"string\"}\n",
        "promptList = []\n",
        "\n",
        "# append the text strings to an array to combine with full prompt later:\n",
        "for x in promptDescriptionsToAppend:\n",
        "  promptList.append(promptStart + \", \" + x + \", \" + promptEnd)\n",
        "\n",
        "# check if we actually wrote enough lines in our text file, if not we'll add empty strings:\n",
        "if len(promptList) != totalSplitCount:\n",
        "  extraLinesNeeded = totalSplitCount - len(promptList)\n",
        "  for x in range(extraLinesNeeded):\n",
        "    promptList.append(promptStart + \", \" + promptEnd)\n",
        "# ------ NOW WE HAVE OUR FULL TEXT FOR THE PROMPTS! ------------------------------\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# now get the input images from the folder we created earlier:\n",
        "# the folder is \"/content/HD/preProcessedGridSplits\"\n",
        "defaultInitPath = \"/content/HD/preProcessedGridSplits/\"\n",
        "\n",
        "# find all the images in the folder meant to read them:\n",
        "numberOfImages = 0\n",
        "init_imgs_list = []\n",
        "for path in os.listdir(defaultInitPath):\n",
        "    if os.path.isfile(os.path.join(defaultInitPath, path)):\n",
        "        numberOfImages += 1\n",
        "        init_imgs_list.append(path)\n",
        "init_imgs_list.sort()\n",
        "#print(numberOfImages)\n",
        "#print(init_imgs_list)\n",
        "\n",
        "# NOW WE HAVE ALL THE VARIABLES WE NEED! we have the list of images, prompts and core numbers like image height and number of tiles along width!\n",
        "# core variables:\n",
        "# promptList (lsit of the full prompts including both custom and appended style strings)\n",
        "# init_imgs_list (list of all the image paths)\n",
        "# totalSplitCount (total number of splits we made = total images to process)\n",
        "# initialImageWidth (in pixels of input image preprocessed)\n",
        "# initialImageHeight (we especially need this for later, this is the input image height in pixels)\n",
        "# numberOfImageAlongX  (check how many split cells go along the X axis)\n",
        "\n",
        "# set up counters and the output folder:\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "# numberPosition = True\n",
        "# opt.saveNumberPosition == numberPosition\n",
        "outputDirectoryProcessed = \"/content/HD/postProcessedGridSplits/\"\n",
        "outputDirectoryFiles = glob.glob('/content/HD/postProcessedGridSplits/**/*', recursive=True)\n",
        "if os.path.exists(outputDirectoryProcessed) == False:\n",
        "  os.makedirs(outputDirectoryProcessed)\n",
        "# clean up existing files in the output Directory:\n",
        "for f in outputDirectoryFiles:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "    except OSError as e:\n",
        "        pass\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# now get to prepare the images, prompts and settings for Stable Diffusion:\n",
        "for num in range((totalSplitCount)):\n",
        "  print(\"Processing image: \" + (str(num+1)) + \" / \" + str(totalSplitCount))\n",
        "  opt.init_img = (\"/content/HD/preProcessedGridSplits/\" + str(init_imgs_list[int(num)]))\n",
        "  opt.ddim_steps = steps\n",
        "  opt.n_iter = 1\n",
        "  opt.n_samples = samples_per_batch\n",
        "  # opt.outdir = os.path.join(outputs_path, batch_name)\n",
        "  opt.outdir = outputDirectoryProcessed\n",
        "  opt.prompt = promptList[int(num)]\n",
        "  opt.sampler = sampler\n",
        "  opt.scale = guidance_scale\n",
        "  opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "  opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "  opt.W, opt.H = map(lambda x: x - x % 64, (width_height[0], width_height[1])) # resize to integer multiple of 64\n",
        "\n",
        "  if opt.init_img != None and opt.init_img != '':\n",
        "      opt.sampler = 'ddim'\n",
        "\n",
        "  if opt.sampler != 'ddim':\n",
        "      opt.ddim_eta = 0.0\n",
        "\n",
        "  # save settings\n",
        "  settings = {\n",
        "      'ddim_eta': ddim_eta,\n",
        "      'guidance_scale': guidance_scale,\n",
        "      'init_image': init_imgs_list[num],\n",
        "      'init_strength': init_strength,\n",
        "      'prompt': promptList[num],\n",
        "      'sampler': sampler,\n",
        "      'samples_per_batch': samples_per_batch,\n",
        "      'seed': opt.seed,\n",
        "      'steps': steps,\n",
        "      'width': opt.W,\n",
        "      'height': opt.H,\n",
        "  }\n",
        "  os.makedirs(opt.outdir, exist_ok=True)\n",
        "  #with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "  #    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "  sample_idx = 0\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  images = generate(opt)\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  print(f\"Used seed: {opt.seed}\")\n",
        "  print(f\"Saved to: {opt.outdir}\")\n",
        "  print(\"Processed image: \" + str((num+1)) + \" / \" + str(totalSplitCount))\n",
        "  display(images[0])\n",
        "\n",
        "  batch_idx += 1\n",
        "  sample_idx += 1\n",
        "# ---------------- end of Stable Diffusion Code-----------------------\n",
        "# now some clean up before extending and merging the images:\n",
        "\n",
        "# clean up existing files again:\n",
        "finalOutputDirectory = \"/content/HD/extendedTiles/\"\n",
        "outputDirectoryFiles = glob.glob('/content/HD/extendedTiles/**/*', recursive=True)\n",
        "if os.path.exists(finalOutputDirectory) == False:\n",
        "  os.makedirs(finalOutputDirectory)\n",
        "# clean up existing files in the output Directory:\n",
        "for f in outputDirectoryFiles:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "    except OSError as e:\n",
        "        pass\n",
        "\n",
        "directoryToRename = \"/content/HD/postProcessedGridSplits/\"\n",
        "directoryToReference = \"/content/HD/preProcessedGridSplits/\"\n",
        "# rename the images to clean them up:\n",
        "listOfImagesToRename = [f for f in listdir(directoryToRename) if f.endswith(\".png\") and isfile(join(directoryToRename, f))]\n",
        "listOfImagesToRename.sort()\n",
        "listOfImagesReference = [f for f in listdir(directoryToReference) if f.endswith(\".png\") and isfile(join(directoryToReference, f))]\n",
        "listOfImagesReference.sort()\n",
        "counter = 0\n",
        "for x in listOfImagesToRename:\n",
        "  newNameSplit = listOfImagesReference[counter].split('pre', 1)\n",
        "  newName = \"post\" + newNameSplit[1]\n",
        "  src =f\"{directoryToRename}/{x}\" # the incorrect file names we need to rename\n",
        "  dst =f\"{directoryToRename}/{newName}\"\n",
        "  os.rename(src, dst)\n",
        "  counter += 1\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 4) After upressing each tile with Stable Diffusion, we 'simply' expand them to full ratio and merge into a final image\n",
        "\n",
        "def add_margin(directory, columnsNumber):\n",
        "    # first gather variable information from the images in our folder:\n",
        "    listOfImages = [f for f in listdir(directory) if f.endswith(\".png\") and isfile(join(directory, f))]\n",
        "    listOfImages.sort()\n",
        "    imgStudy = Image.open(directory + '/' + listOfImages[0]) # load the image\n",
        "    w, h = imgStudy.size\n",
        "    imageWidth = w # could be 512, 640, etc, it reads the actual image width so we don't guess\n",
        "    numberOfImages = len(listOfImages) # total images in the folder we're reading\n",
        "    extendAmount = int(imageWidth / 2) # example; if the image width is 512, then this extend amount is 256\n",
        "    # extensionWidth = int(imageWidth * (columnsNumber / 2)) # the final image width, example 2048\n",
        "    # extensionHeight = int((numberOfImages / (columnsNumber)) * extendAmount) # the final image height, example 3072\n",
        "    \n",
        "    # these values come from the actual image, so it's more legit:\n",
        "    # since we have a feature called gridRatio which actually changes the final width, we'll have to accomodate new potential sizes:\n",
        "    if imageWidth == 512:\n",
        "      extensionWidth = initialImageWidth\n",
        "      extensionHeight = initialImageHeight\n",
        "    else:\n",
        "      pixelDensityIncrease = imageWidth / 512\n",
        "      extensionWidth = int(initialImageWidth * pixelDensityIncrease)\n",
        "      extensionHeight = int(initialImageHeight * pixelDensityIncrease) \n",
        "\n",
        "    # print(extensionHeight)\n",
        "\n",
        "    # next create variables for extending / adding transparency to the images:\n",
        "    color =  (0, 0, 0, 0) # the last number, 0 means the value of the extension will be completely transparent if we save as .png\n",
        "    extendTop = 0 # we start at zero since we start from the top left corner\n",
        "    extendBottom = int((extensionHeight - imageWidth) + imageWidth) # we start with a value since there is space on the bottom in the 1st image; example 2560\n",
        "    extendLeft = 0 # we start at zero since we start from the top left corner\n",
        "    extendLeft = int((imageWidth / 2) - imageWidth) # we actually start in the negative since we begin by adding + 256 to the left, which brings us to 0 initially\n",
        "    extendRight = int(extensionWidth +  (imageWidth/2)) # we start with a value since there is space on the right in the 1st image; example 2304, then subtract 256\n",
        "\n",
        "    # we have to check which image we're on, both in total, by column and by row:\n",
        "    image_counter = 0\n",
        "    countW = 1\n",
        "    countH = 0\n",
        "\n",
        "    # these values controls how the mask looks and we also need to gather the resulting img + mask for when we stitch them back together:\n",
        "    maskBorderAmount = 96\n",
        "    maskBlurIntensity = 16\n",
        "    listOfExportedImages = []\n",
        "    listOFExportedImageMasks = []\n",
        "\n",
        "    # before processing the images, check if a folder to store the new images exists:\n",
        "    # filePathSave = directory + \"\\\\\" + \"extendedFrame\" + \"\\\\\"\n",
        "    filePathSave = \"/content/HD/extendedTiles/\"\n",
        "    isExist = os.path.exists(filePathSave)\n",
        "    if not isExist:\n",
        "        os.makedirs(filePathSave)    \n",
        "\n",
        "    # now cycle through the images and extend them, etc:\n",
        "    for image in listOfImages:\n",
        "        # read and number the image:\n",
        "        image_counter += 1\n",
        "        image_counterPadded = str(image_counter).zfill(4) # the number we label each image is padded; example 0001\n",
        "        img = Image.open(directory + '/' + image) # load the image\n",
        "        img = img.convert(\"RGBA\") # make sure this image supports transparency\n",
        "\n",
        "        # calculate how much to extend the canvas in all directions:\n",
        "        if countW <= columnsNumber: # if the count is lower than the number of images width wise, we only need to extend on the width \n",
        "            countW += 1\n",
        "            extendLeft += int(imageWidth / 2) # example: 256\n",
        "            extendRight -= int(imageWidth / 2)\n",
        "            # print(\"exension amount left: \" + str(extendLeft))\n",
        "        elif countW > columnsNumber:\n",
        "            # once we exceed the number of images width wise, we reset the count/width and change the spacing just once for the height\n",
        "            countW = 2\n",
        "            countH +=1\n",
        "            extendTop += int(imageWidth / 2) # example: 256\n",
        "            extendBottom -= int(imageWidth / 2)\n",
        "            extendLeft = 0\n",
        "            extendRight = extensionWidth\n",
        "\n",
        "        # finally calculate how to edit the dimensions of the transparent bits of the image:\n",
        "        # width, height = img.size\n",
        "        new_width = extendRight + extendLeft\n",
        "        new_height = extendTop + extendBottom\n",
        "        result = Image.new(img.mode, (new_width, new_height), color)\n",
        "        result.paste(img, (extendLeft, extendTop))\n",
        "        \n",
        "        # count which actual column and row we're on:\n",
        "        image_rowPadded = str(countH+1).zfill(3)\n",
        "        image_columnPadded = str(countW-1).zfill(3)\n",
        "\n",
        "        # create a feather mask around the cropped image to blend it in better when compositing:\n",
        "        mask_im = Image.new(\"L\", result.size, 0)\n",
        "        draw = ImageDraw.Draw(mask_im)\n",
        "        # draw.rectangle((extendLeft+ 12, extendTop + 12, extendRight - (extendRight-imageWidth) -18, extendBottom - (extendBottom - imageWidth)-18), fill=255)\n",
        "        #maskX = int((imageWidth/2) * (countW - 2) + maskBorderAmount)\n",
        "        #maskY = int((imageWidth/2) * (countH) + maskBorderAmount)\n",
        "        if countW == 2:\n",
        "            maskX = int((imageWidth/2) * (countW - 2))\n",
        "        else:\n",
        "            maskX = int((imageWidth/2) * (countW - 2) + maskBorderAmount)\n",
        "        if countH == 0:\n",
        "            maskY = int((imageWidth/2) * (countH))\n",
        "        else:\n",
        "            maskY = int((imageWidth/2) * (countH) + maskBorderAmount)\n",
        "        maskW = int(((imageWidth/2) * (countW - 2)) + imageWidth - maskBorderAmount)\n",
        "        maskH = int(((imageWidth/2) * (countH)) + imageWidth - maskBorderAmount)\n",
        "        draw.rectangle((maskX, maskY, maskW, maskH), fill=255) # THIS is where we draw the MASK, fill=255 must mean it's a perfect white? I forgot\n",
        "        # create a blur for the mask:\n",
        "        mask_im_blur = mask_im.filter(ImageFilter.GaussianBlur(maskBlurIntensity)) # maskBlurIntensity is the variable we define to control how intense to mask\n",
        "\n",
        "        # finally, save this image in a new folder for the extended canvas version:\n",
        "        # filePathSaveFull = filePathSave + \"paddedImage\" + \"_\" + image_counterPadded + \".png\"\n",
        "        filePathSaveFull = filePathSave + \"paddedImage\" + \"_\" + image_rowPadded + \"_\" + image_columnPadded + \".png\"\n",
        "        result.save(filePathSaveFull, quality=100)\n",
        "        listOfExportedImages.append(result)\n",
        "        # save the mask too (actually I rather not save the mask, these are easy to make in Photoshop and this saves time not saving):\n",
        "        # filePathSaveFull = filePathSave + \"mask\" + \"_\" + image_counterPadded + \".png\"\n",
        "        # mask_im_blur.save(filePathSaveFull, quality=100)\n",
        "        listOFExportedImageMasks.append(mask_im_blur) # instead of saving the masks, save them to a variable\n",
        "        output.clear()\n",
        "        print(\"Extended: \" + str(image_counter) + \" / \" + str(len(listOfImages)))\n",
        "\n",
        "    # after extending the frames and creating the masks, combine them into a single image:\n",
        "    back_img = listOfExportedImages[0].copy()\n",
        "    countMask = 0\n",
        "    filePathSaveFull = \"/content/HD/\" + \"_final_HD\" + \".png\"\n",
        "\n",
        "    for image in listOfExportedImages:\n",
        "        #listOfExportedImages = []\n",
        "        #listOFExportedImageMasks = []\n",
        "        if image == listOfExportedImages[0]:\n",
        "            back_img.save(filePathSaveFull, quality=100)\n",
        "        else:\n",
        "            #back_img = Image.open(filePathSaveFull) # load the image\n",
        "            back_img.paste(image, (0,0), listOFExportedImageMasks[countMask])\n",
        "            #back_img.save(filePathSaveFull, quality=100)\n",
        "        countMask += 1\n",
        "        output.clear()\n",
        "        print(\"Processed: \" + str(countMask) + \" / \" + str(len(listOfExportedImages)))\n",
        "    back_img.save(filePathSaveFull, quality=100)\n",
        "    print(\"done\")\n",
        "\n",
        "# img_new = add_margin(r\"H:\\New\\testUpres\\testCodeHD\", 512, 8)\n",
        "img_new = add_margin(r\"/content/HD/postProcessedGridSplits\", numberOfImageAlongX) # you need to include input folder, make sure only the # of images you want are in there and include # of total images in width\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "# Now we copy the final files to Google Drive to back them up:\n",
        "print(\"Wait while copying to Google Drive...\")\n",
        "import shutil\n",
        "copyAddressOnDrive = \"/content/gdrive/MyDrive/AI/StableDiffusionHD/\" + batch_name\n",
        "\n",
        "directoryToCopy = \"/content/HD/extendedTiles/\"\n",
        "listOfImagesToCopy = [f for f in listdir(directoryToCopy) if f.endswith(\".png\") and isfile(join(directoryToCopy, f))]\n",
        "listOfImagesToCopy.sort()\n",
        "\n",
        "isExist = os.path.exists(copyAddressOnDrive + \"/tiles/\")\n",
        "if not isExist:\n",
        "    os.makedirs(copyAddressOnDrive + \"/tiles/\")\n",
        "\n",
        "for x in listOfImagesToCopy:\n",
        "  fullPath = directoryToCopy + x\n",
        "  shutil.copy(fullPath, copyAddressOnDrive + \"/tiles/\")\n",
        "\n",
        "shutil.copy(\"/content/HD/_final_HD.png\", copyAddressOnDrive)\n",
        "print(\"Done copying files. Enjoy!\")"
      ],
      "metadata": {
        "id": "WT-KgMa_qvKd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <u> B) Post Processing - **Face Recognition Upscaling** </u> "
      ],
      "metadata": {
        "id": "vqCQrrKma9HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put each of your prompts on a new line:\n",
        "# NOTE: you must run this cell once at start up and whenever you make an edit!\n",
        "promptsListForOpenCV = '''\n",
        "blue skies\n",
        "female magician\n",
        "attractive American Woman\n",
        "'''"
      ],
      "metadata": {
        "id": "xfUtoOSRjyYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PvTzaOl9pw4W"
      },
      "outputs": [],
      "source": [
        "#@title Face Recognition & Upscale (Run):\n",
        "#@markdown `This script will process all the images in the imgPath location, sequentially by name`<br>\n",
        "#@markdown `Turn on useRealESRGANforUpscale for higher quality results, just takes longer`<br>\n",
        "#@markdown `If it fails, the ESRGAN scale may be too high, your input image is too high res or the grid ratio is too high`<br>\n",
        "#@markdown `Predictable low end results: input images size 640x640, scale factor 2, grid ratio 672, init_strength 0.5`<br>\n",
        "\n",
        "##@markdown\n",
        "\n",
        "import re\n",
        "import random\n",
        "# facial recognition code:\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "from base64 import b64decode\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import glob\n",
        "import shutil\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# Create the folders & folder variables we need: -----------\n",
        "ProjectName = \"testFaceRecognitionC\" #@param {type:\"string\"}\n",
        "# the folder to contain all the other folders:\n",
        "FRpathBase = \"/content/facialRecognition/\"\n",
        "os.makedirs(FRpathBase, exist_ok=True) \n",
        "# made a project for everything as well, why not:\n",
        "FRpathProject = \"/content/facialRecognition/\" + ProjectName + \"/\"\n",
        "os.makedirs(FRpathProject, exist_ok=True) \n",
        "# where the study material (ex previewing how the crops will look) are saved\n",
        "FRpathProjectWIP = \"/content/facialRecognition/\" + ProjectName + \"/workFiles/\"\n",
        "os.makedirs(FRpathProjectWIP, exist_ok=True) \n",
        "# where the pre-gen crops will be saved:\n",
        "FRpathProjectCrops = \"/content/facialRecognition/\" + ProjectName + \"/crops/\"\n",
        "os.makedirs(FRpathProjectCrops, exist_ok=True) \n",
        "# where the newly generated but still cropped faces will be saved:\n",
        "FRpathProjectUprezzedCrops = \"/content/facialRecognition/\" + ProjectName + \"/uprezzedCrops/\"\n",
        "os.makedirs(FRpathProjectCrops, exist_ok=True) \n",
        "# where the final upscaled and face swapped images will be saved:\n",
        "FRpathProjectFinal = \"/content/facialRecognition/\" + ProjectName + \"/finals/\"\n",
        "os.makedirs(FRpathProjectFinal, exist_ok=True) \n",
        "# where the real-ESRGAN upscaled images will be temp saved:\n",
        "FRpathProjectUpscaledINIT = \"/content/facialRecognition/\" + ProjectName + \"/upscaledINIT/\"\n",
        "os.makedirs(FRpathProjectUpscaledINIT, exist_ok=True) \n",
        "# where the extended transparent results are stored:\n",
        "FRpathProjectExtended = \"/content/facialRecognition/\" + ProjectName + \"/extended/\"\n",
        "os.makedirs(FRpathProjectExtended, exist_ok=True) \n",
        "\n",
        "# clean up the work files (only clean up the base directory):\n",
        "outputDirectoryProcessed = FRpathBase\n",
        "outputDirectoryFiles = glob.glob(FRpathBase + '/**/*', recursive=True)\n",
        "if os.path.exists(outputDirectoryProcessed) == False:\n",
        "  os.makedirs(outputDirectoryProcessed)\n",
        "# clean up existing files in the output Directory:\n",
        "for f in outputDirectoryFiles:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "    except OSError as e:\n",
        "        pass\n",
        "\n",
        "# save the positions to lists to call later:\n",
        "countOfFacesPerInput = []\n",
        "positionFaceImageXList = []\n",
        "positionFaceImageYList = []\n",
        "positionFaceImageWList = []\n",
        "positionFaceImageHList = []\n",
        "expandFacialRecognitionRatio = 15 \n",
        "\n",
        "# A) Check the Image for Faces and crop it: -------------------------------------------\n",
        "imgPath = \"/content/init_image/\" #@param {type:\"string\"}\n",
        "imgPathInput = [f for f in listdir(imgPath) if isfile(join(imgPath, f))]\n",
        "imgPathInput.sort # this variable gives a list of all the images in the init_image folder (ex. [img1.png, img2.png])\n",
        "numberOfImagesToProcess = len(imgPathInput)\n",
        "counter = 0\n",
        "useRealESRGANforUpscale = True #@param {type:\"boolean\"}\n",
        "ESRGANscaleFactor = 3 #@param {type:\"slider\", min:2, max:4, step:0.5}\n",
        "#@markdown `for faceRecognitionMinimizer, higher values = less faces detected. Default is 3-5`<br>\n",
        "faceRecognitionMinimizer = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Real-ESRGAN Upscaling for the Input Images:\n",
        "# since it takes a while to upscale, make this feature optional:\n",
        "if useRealESRGANforUpscale == True:\n",
        "  # it seems only png work, so convert .jpg to PNG:\n",
        "  for img in imgPathInput:\n",
        "    if img.endswith('.jpg'):\n",
        "      im1 = Image.open(imgPath + img)\n",
        "      splitPath = os.path.splitext(imgPath + img)[0]\n",
        "      newPath = splitPath + '.png'\n",
        "      im1.save(newPath)\n",
        "      os.remove(imgPath + img)\n",
        "  imgPathInput = [f for f in listdir(imgPath) if isfile(join(imgPath, f))]\n",
        "  imgPathInput.sort # this variable gives a list of all the images in the init_image folder (ex. [img1.png, img2.png])\n",
        "\n",
        "  #1) set up the file to load into the ESRGAN:\n",
        "  # note: these folders live inside the real ESRGAN created folders, which is why they do not have /content/ in their name\n",
        "  upload_folder = 'upload' # wierd directories that ESRGAN uses\n",
        "  result_folder = 'results'# ignore the lack of /content/ directory\n",
        "\n",
        "  # create the upload and results folders + clean them up:\n",
        "  if os.path.isdir(upload_folder):\n",
        "      shutil.rmtree(upload_folder)\n",
        "  if os.path.isdir(result_folder):\n",
        "      shutil.rmtree(result_folder)\n",
        "  os.mkdir(upload_folder)\n",
        "  os.mkdir(result_folder)\n",
        "\n",
        "  # upload images: (in this case we read our init_images)\n",
        "  uploadPath = imgPath\n",
        "  filesList = imgPathInput\n",
        "  for filename in filesList:\n",
        "    dst_path = os.path.join(upload_folder, filename)\n",
        "    print(f'copy {filename} to {dst_path}')\n",
        "    shutil.copy(uploadPath + filename, dst_path)\n",
        "\n",
        "  # 2) Core Code to Make ESRGAN Uprez the Image:\n",
        "  # if it is out of memory, try to use the `--tile` option\n",
        "  # We upsample the image with the scale factor X3.5\n",
        "  if ESRGANscaleFactor == 2:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 2 --face_enhance\n",
        "  elif ESRGANscaleFactor == 2.5:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 2.5 --face_enhance  \n",
        "  elif ESRGANscaleFactor == 3:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 3 --face_enhance\n",
        "  elif ESRGANscaleFactor == 3.5:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 3.5 --face_enhance\n",
        "  elif ESRGANscaleFactor == 4:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 4 --face_enhance\n",
        "  # Arguments\n",
        "  # -n, --model_name: Model names\n",
        "  # -i, --input: input folder or image\n",
        "  # --outscale: Output scale, can be arbitrary scale factore. \n",
        "  input_folder = '/content/stable-diffusion/Real-ESRGAN/upload'\n",
        "  result_folder = '/content/stable-diffusion/Real-ESRGAN/results'\n",
        "  input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n",
        "  output_list = sorted(glob.glob(os.path.join(result_folder, '*')))\n",
        "  output_list.sort()\n",
        "  #print(input_list)\n",
        "  #print(output_list) # list of upressed files\n",
        "\n",
        "  # now we replace the original variables with our new images:\n",
        "  imgPath = result_folder + \"/\"\n",
        "  imgPathInput = [f for f in listdir(imgPath) if isfile(join(imgPath, f))]\n",
        "  imgPathInput.sort # this variable gives a list of all the images in the init_image folder (ex. [img1.png, img2.png])\n",
        "  time.sleep(1)\n",
        "  # copy the files to easer to see location:\n",
        "  for item in imgPathInput:\n",
        "    fullPath = imgPath + item\n",
        "    shutil.copy(fullPath, FRpathProjectUpscaledINIT)\n",
        "\n",
        "  imgPath = FRpathProjectUpscaledINIT\n",
        "  imgPathInput = [f for f in listdir(imgPath) if isfile(join(imgPath, f))]\n",
        "  imgPathInput.sort # this variable gives a list of all the images in the init_image folder (ex. [img1.png, img2.png])\n",
        "# print(\"imgPathInput:\" + str(imgPathInput))\n",
        "  # ------- END real ESRGAN -----------------\n",
        "\n",
        "\n",
        "\n",
        "checkIfRealFace = 0\n",
        "# -------------------------------------------------------------------------------\n",
        "# -------------------------FACIAL RECOGNITION: ----------------------------------\n",
        "# A) Check the Image for Faces and crop it: -------------------------------------------\n",
        "for image in imgPathInput:\n",
        "  counter +=1\n",
        "  counterString = str(counter)\n",
        "  inputImageFullPath = imgPath + image\n",
        "  imgPathOutput = FRpathProjectWIP + \"study_\" + counterString.zfill(4) + \".png\" # ex. study1.png\n",
        "  # print(imgPathInput)\n",
        "  # we need to save the x and y coordinates of the face image so we can swap it out later:\n",
        "  # we also need to save the length and width:\n",
        "\n",
        "  positionFaceImageX = 0\n",
        "  positionFaceImageY = 0\n",
        "  positionFaceImageW = 0\n",
        "  positionFaceImageH = 0\n",
        "\n",
        "  # 1) read the image from path:\n",
        "  img = cv2.imread(inputImageFullPath)\n",
        "  imgCopy = img.copy() # we create a copy of the input image to purform the cropping on since we use the original link to create the study images\n",
        "  fullImageHeight, fullImageWidth, channels  = img.shape # we need to know the full image width to know the ratio to expand the recognition\n",
        "\n",
        "  # 2) convert image to grayscale (required by image recognition model):\n",
        "  gray_img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # 3) Use the Haar Cascade model to detect faces from the image:\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "  eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "  # it seems higher resolution images requires us to modify the intensity of searching for faces, which is this value:\n",
        "  if useRealESRGANforUpscale == False:\n",
        "    faces = face_cascade.detectMultiScale(gray_img,1.1,faceRecognitionMultiplier)\n",
        "  elif useRealESRGANforUpscale == True:\n",
        "    faces = face_cascade.detectMultiScale(gray_img,1.1,faceRecognitionMultiplier*2)\n",
        "\n",
        "  # 4) print the number of faces detected:\n",
        "  #print(f\"len(faces)} faces detected in the image.\")\n",
        "  numberOfFacesDetected = len(faces)\n",
        "  counterFaces = 0\n",
        "  countOfFacesPerInput.append(numberOfFacesDetected)\n",
        "  print(\"Number of faces detected:\" + str(numberOfFacesDetected))\n",
        "\n",
        "  # 5a) Draw rectangle around the faces and crop the faces:\n",
        "  # 5b) also save the correct position data (the distance of top left corner + crop W & H):\n",
        "  for f in faces:\n",
        "    counterFaces +=1\n",
        "    counterFacesString = str(counterFaces)\n",
        "    x1, y1, w1, h1 = [ v for v in f ]\n",
        "    calculateDistancePixelsPercentage = int(fullImageWidth / expandFacialRecognitionRatio)\n",
        "    calculateDistancePixels = int(calculateDistancePixelsPercentage)\n",
        "    positionFaceImageX = x = int(x1 - calculateDistancePixels)\n",
        "    positionFaceImageY = y = int(y1 - calculateDistancePixels)\n",
        "    positionFaceImageW = w = int(w1 + (calculateDistancePixels * 2))\n",
        "    positionFaceImageH = h = int(h1 + (calculateDistancePixels * 2))\n",
        "    positionFaceImageXList.append(positionFaceImageX)\n",
        "    positionFaceImageYList.append(positionFaceImageY)\n",
        "    positionFaceImageWList.append(positionFaceImageW)\n",
        "    positionFaceImageHList.append(positionFaceImageH)\n",
        "\n",
        "    sub_face = imgCopy[y:y+h, x:x+w]\n",
        "\n",
        "    fname, ext = os.path.splitext(inputImageFullPath)\n",
        "    # cv2.rectangle(img, (x,y), (x+w,y+h), (256,256,256), thickness=20) # FOR DEBUGGING IF THE IMAGE IS BEING PLACED CORRECTLY BACK ON THE FULL IMG\n",
        "    #cv2.imwrite(\"/content/known/\" + fname+\"_cropped_\"+ext, sub_face)\n",
        "    #cv2.imwrite(FRpathProjectCrops + \"cropped\" + \".png\", sub_face)\n",
        "    try:\n",
        "      cv2.imwrite(FRpathProjectCrops + \"cropped_\" + counterString.zfill(4) + \"_f\" + counterFacesString.zfill(2) + \".png\", sub_face)\n",
        "      cv2.rectangle(img, (x,y), (x+w,y+h), (256,256,256), thickness=2)\n",
        "      cv2.rectangle(img, (x1, y1), (x1 + w1, y1+h1), color=(0, 256, 0), thickness=1) \n",
        "      #break # only process the first face result, if there are multiple faces those get skipped\n",
        "      checkIfRealFace += 1\n",
        "    except:\n",
        "      \"not a face!\"\n",
        "\n",
        "  # save the image with rectangles\n",
        "  cv2.imwrite(imgPathOutput, img) # the study image\n",
        "\n",
        "# since we may have more than one face in the images, calculate the total # of images overall:\n",
        "numberOfFacesDetected = checkIfRealFace\n",
        "\n",
        "totalNumberOFCrops = 0\n",
        "for x in countOfFacesPerInput:\n",
        "  totalNumberOFCrops += x\n",
        "\n",
        "# ----------------------END FACIAL RECOGNITION P1 -------------------------------\n",
        "# -------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "# B) Run Stable Diffusion to Process Newly Generated Image:\n",
        "\n",
        "\n",
        "#B1) first deal with getting the input images and the prompts:\n",
        "promptList = []\n",
        "promptList = [\"an attract female portrait by artgerm, greg rutkowski, trending on artstation, detailed, rendered in Octane, ethereal\"]\n",
        "\n",
        "# create a list of all the prompts from new line:\n",
        "if type(promptsListForOpenCV) is str:\n",
        "  promptsListForOpenCV = [i for i in promptsListForOpenCV.split('\\n') if i]\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 3) Now we can run these prompts and images through Stable Diffusion to Upres them!\n",
        "\n",
        "\n",
        "#@markdown Basic settings:\n",
        "batch_name = ProjectName\n",
        "gridRatio = \"512\" #@param [512, 608, 640, 672, 704]\n",
        "width_height = [int(gridRatio), int(gridRatio)] #param{type: 'raw'}\n",
        "guidance_scale = 25 #@param {type:\"number\"}\n",
        "steps = 7 #@param {type:\"integer\"}\n",
        "init_strength = 0.4 #@param {type:\"number\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "num_batch_images = 1 #param {type:\"integer\"}\n",
        "sampler = 'ddim' #@param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 #param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"integer\"}\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# now get the input images from the folder we created earlier:\n",
        "\n",
        "# find all the images in the folder meant to read them:\n",
        "numberOfImages = 0\n",
        "init_imgs_list = []\n",
        "for path in os.listdir(FRpathProjectCrops): # note: imgPath is defined as a parameter at the beginning of the code\n",
        "    if os.path.isfile(os.path.join(FRpathProjectCrops, path)):\n",
        "        numberOfImages += 1\n",
        "        init_imgs_list.append(path)\n",
        "init_imgs_list.sort()\n",
        "#print(numberOfImages)\n",
        "#print(init_imgs_list)\n",
        "\n",
        "# --------------- Next we deal with Prompts: -------------\n",
        "#@markdown Prompt\n",
        "promptStart = \"female\" #@param {type:\"string\"}\n",
        "promptEnd = \"portrait, young woman, detailed gorgeous face, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by artgerm and greg rutkowski and alphonse mucha\" #@param {type:\"string\"}\n",
        "promptList = []\n",
        "promptListFullString = []\n",
        "\n",
        "# append the text strings to an array to combine with full prompt later:\n",
        "for x in promptsListForOpenCV:\n",
        "  # this one will just have the prompts we have written + the prompt template, it will not duplicate prompts over multi face images yet\n",
        "  promptListFullString.append(promptStart + \", \" + x + \", \" + promptEnd)\n",
        "\n",
        "# check if we actually wrote enough lines in our text file, if not we'll add empty strings:\n",
        "if len(promptListFullString) != numberOfImagesToProcess: # number of images to process is the total number of INIT images\n",
        "  extraLinesNeeded = numberOfImagesToProcess - len(promptListFullString)\n",
        "  for x in range(extraLinesNeeded):\n",
        "    promptListFullString.append(promptStart + \", \" + promptEnd)\n",
        "\n",
        "counter = 0\n",
        "# now we check how many of these lines we need to duplicate for multi face images:\n",
        "for x in countOfFacesPerInput: # this tells us how many faces are in per image; ex [0, 1, 6]\n",
        "  if x == 0:\n",
        "    # if there are no faces in the image, skip appending that line\n",
        "    counter += 1\n",
        "  else:\n",
        "    for y in range (x):\n",
        "      promptList.append(promptListFullString[counter]) # append this line as many times as there are faces in the image\n",
        "    counter += 1 # after appending all the neccessary lines, move on to the next line\n",
        "\n",
        "#print(len(promptList))\n",
        "#for x in promptList:\n",
        "#  print(x)\n",
        "# ------ NOW WE HAVE OUR FULL TEXT FOR THE PROMPTS! ----------------------------\n",
        "#print(len(init_imgs_list))\n",
        "#print(init_imgs_list)\n",
        "\n",
        "\n",
        "\n",
        "# set up counters and the output folder:\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "# numberPosition = True\n",
        "# opt.saveNumberPosition == numberPosition\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# now get to prepare the images, prompts and settings for Stable Diffusion:\n",
        "for num in range((totalNumberOFCrops)):\n",
        "  print(\"Processing image: \" + (str(num+1)) + \" / \" + str(totalNumberOFCrops))\n",
        "  if len(init_imgs_list) == 0:\n",
        "    print(\"No faces detected! Exiting\")\n",
        "    break\n",
        "  opt.init_img = (FRpathProjectCrops + str(init_imgs_list[int(num)]))\n",
        "  opt.ddim_steps = steps\n",
        "  opt.n_iter = 1\n",
        "  opt.n_samples = samples_per_batch\n",
        "  # opt.outdir = os.path.join(outputs_path, batch_name)\n",
        "  opt.outdir = FRpathProjectUprezzedCrops\n",
        "  opt.prompt = promptList[int(num)]\n",
        "  opt.sampler = sampler\n",
        "  opt.scale = guidance_scale\n",
        "  opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "  opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "  opt.W, opt.H = map(lambda x: x - x % 64, (width_height[0], width_height[1])) # resize to integer multiple of 64\n",
        "\n",
        "  if opt.init_img != None and opt.init_img != '':\n",
        "      opt.sampler = 'ddim'\n",
        "\n",
        "  if opt.sampler != 'ddim':\n",
        "      opt.ddim_eta = 0.0\n",
        "\n",
        "  # save settings\n",
        "  settings = {\n",
        "      'ddim_eta': ddim_eta,\n",
        "      'guidance_scale': guidance_scale,\n",
        "      'init_image': init_imgs_list[num],\n",
        "      'init_strength': init_strength,\n",
        "      'prompt': promptList[num],\n",
        "      'sampler': sampler,\n",
        "      'samples_per_batch': samples_per_batch,\n",
        "      'seed': opt.seed,\n",
        "      'steps': steps,\n",
        "      'width': opt.W,\n",
        "      'height': opt.H,\n",
        "  }\n",
        "  os.makedirs(opt.outdir, exist_ok=True)\n",
        "  #with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "  #    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "  sample_idx = 0\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  images = generate(opt)\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  print(f\"Used seed: {opt.seed}\")\n",
        "  print(f\"Saved to: {opt.outdir}\")\n",
        "  print(\"Processed image: \" + str((num+1)) + \" / \" + str(totalNumberOFCrops))\n",
        "  display(images[0])\n",
        "\n",
        "  batch_idx += 1\n",
        "  sample_idx += 1\n",
        "\n",
        "# ------- END OF STABLE DIFFUSION PROCESSING ------------------\n",
        "\n",
        "# Clean up the files in /uprezzedCrops/\n",
        "# now some clean up before merging the cropped face images into the original images:\n",
        "\n",
        "# FRpathProjectFinal # /content/facialRecognition/testFacialRecognition/finals\n",
        "\n",
        "directoryToRename = FRpathProjectUprezzedCrops # /content/facialRecognition/testFacialRecognition/uprezzedCrops\n",
        "# rename the images to clean them up:\n",
        "listOfImagesToRename = [f for f in listdir(directoryToRename) if f.endswith(\".png\") and isfile(join(directoryToRename, f))]\n",
        "listOfImagesToRename.sort()\n",
        "counter = 1\n",
        "listOfImagesToIntegrate = []\n",
        "for x in listOfImagesToRename:\n",
        "  counterString = str(counter)\n",
        "  # newName = \"result_\" + counterString.zfill(4) + \".png\"\n",
        "  newName = \"result_\" + counterString.zfill(4) + \".png\"\n",
        "  src =f\"{directoryToRename}/{x}\" # the incorrect file names we need to rename\n",
        "  dst =f\"{directoryToRename}/{newName}\"\n",
        "  os.rename(src, dst)\n",
        "  counter += 1\n",
        "\n",
        "listOfImagesToIntegrate = [f for f in listdir(directoryToRename) if f.endswith(\".png\") and isfile(join(directoryToRename, f))]\n",
        "listOfImagesToIntegrate.sort()\n",
        "# end clean up\n",
        "\n",
        "\n",
        "# ------- RESIZING AND STITCHING CODE TO FINISH IT OFF: ---------------\n",
        "counter = 0\n",
        "selectInitialImage = 0\n",
        "countRepeatPrompts = 0\n",
        "tempCollectExtendedImg = []\n",
        "imgPathOutputFin = \"\"\n",
        "for image in listOfImagesToIntegrate:\n",
        "  # RESIZE IMAGES: -------------------------------------------\n",
        "  # since we generate the close up crops in a larger resolution, we have to unify them with the original images\n",
        "\n",
        "  # first gather information from the images in our folders:\n",
        "  imgSizeCropped = Image.open(FRpathProjectCrops + '/' + init_imgs_list[counter]) # load the image from /crops/\n",
        "  w, h = imgSizeCropped.size # shoud be on the small side\n",
        "  imageWidthCropped = w\n",
        "\n",
        "  imgSizeGenerated = Image.open(FRpathProjectUprezzedCrops + '/' + listOfImagesToIntegrate[counter]) # load the image from /uprezzedCrops/\n",
        "  w, h = imgSizeGenerated.size # should be 512 pixels +\n",
        "  imageWidthGenerated = w # could be 512, 640, etc, it reads the actual image width so we don't guess\n",
        "\n",
        "  pixelDensityIncrease = imageWidthGenerated / imageWidthCropped # could be 1.25 which is 25% larger\n",
        " \n",
        "  if useRealESRGANforUpscale == False:\n",
        "    # easy way to unify: downscale the upscaled crop to fit the original crop:\n",
        "    imagePathFull = Image.open(FRpathProjectUprezzedCrops + image)\n",
        "    new_image = imagePathFull.resize((imageWidthCropped, imageWidthCropped))\n",
        "  elif useRealESRGANforUpscale == True:\n",
        "    # harder way to unify; upscale\n",
        "    # although since we used this variable to swap the original input images for the upscaled ones, it might be easier than I think\n",
        "    imagePathFull = Image.open(FRpathProjectUprezzedCrops + image)\n",
        "    new_image = imagePathFull.resize((imageWidthCropped, imageWidthCropped))\n",
        "\n",
        "  #print(image)\n",
        "  #print(imgPathInput)\n",
        "  #print(imgPath + imgPathInput[counter])\n",
        "  # -------------------------------------------------------------------------------\n",
        "  # -------------------------FACE RECOGNITION: ----------------------------------\n",
        "  # D) merge cropped image back to the original: ---------------------------------------------------\n",
        "\n",
        "  # this code will take 2 images (1 full and 1 crop) and paste the cropped image at proper coordinates\n",
        "  #Read the two images:\n",
        "  # image1 = Image.open('/content/known/y1.png')\n",
        "  # image1 = Image.open(imgPath + imgPathInput[counter])\n",
        "  # to get image1 which is the full image which may not correspond to the crop #, we have to determine which initial images should go with this crop:\n",
        "  currentInitialImageWork = countOfFacesPerInput[selectInitialImage] # example: [0, 1, 6]\n",
        "  # select which initial image we're on:\n",
        "  while currentInitialImageWork == 0:\n",
        "    if currentInitialImageWork == 0:\n",
        "      # this image has no faces, so we should not process on this initial image\n",
        "      selectInitialImage += 1\n",
        "      currentInitialImageWork = countOfFacesPerInput[selectInitialImage] # example: [0, 1, 6]\n",
        "    if currentInitialImageWork != 0: # if this image has a face, we can stop looking for an image with a face\n",
        "      break\n",
        "  image1 = Image.open(imgPath + imgPathInput[selectInitialImage]) # the initial base image\n",
        "  countRepeatPrompts += 1\n",
        "  image2 = new_image # the cropped image\n",
        "  # image2 = image2.convert(\"RGBA\") # make sure the image supports transparency\n",
        "  # now we have the images to process with ----\n",
        "\n",
        "  # create a feather mask around the cropped image to blend it in better:\n",
        "  mask_im = Image.new(\"L\", image2.size, 0)\n",
        "  draw = ImageDraw.Draw(mask_im)\n",
        "  x, y = image2.size # height and width of the crop image\n",
        "  distance = y / 15 # framing of mask that will contain the blur region\n",
        "  draw.rectangle((distance, distance, positionFaceImageWList[counter]-distance, positionFaceImageHList[counter]-distance), fill=255)\n",
        "  # create a blur for the mask:\n",
        "  mask_im_blur = mask_im.filter(ImageFilter.GaussianBlur(4))\n",
        "  # mask_im_blur.save('/content/mask_rectangle.png')\n",
        "\n",
        "  counterString = str(counter+1)\n",
        "  # now we output this image:\n",
        "  new_image1 = image1.copy()\n",
        "  x, y = new_image1.size # height and width of the base image\n",
        "  new_imageTransparent = Image.new(\"RGBA\",(x,y),(0,0,0,0)).convert(\"RGBA\") # full transparency of the input image\n",
        "  new_imageBlack = Image.new(\"RGBA\",(x,y),(0,0,0)) # fully black of the input image\n",
        "  new_imageTransparentBlend = new_imageTransparent.copy()\n",
        "\n",
        "  stringImageNumber = str(selectInitialImage+1)\n",
        "  stringcountRepeatPrompts = str(countRepeatPrompts)\n",
        "  imgPathOutput = FRpathProjectExtended + \"extended_\" + stringImageNumber.zfill(4) + \"_f\" + stringcountRepeatPrompts.zfill(2) + \".png\" # ex. finished_0001.png\n",
        "  # note: the position X and Y variables must be calculated earlier!\n",
        "\n",
        "  new_image = image1.copy()\n",
        "  new_image.paste(image2, (positionFaceImageXList[counter],positionFaceImageYList[counter]), mask_im_blur)\n",
        "  new_imageTransparent.paste(image2, (positionFaceImageXList[counter],positionFaceImageYList[counter]))\n",
        "    \n",
        "  #new_image.save(imgPathOutput) # new_image is the full image\n",
        "  new_imageTransparent.save(imgPathOutput) # the transparent cropped images\n",
        "\n",
        "  tempCollectExtendedImg.append(new_imageTransparent) # add this new extended crop to an array so we can apply it to the full image after processing it\n",
        "  counter +=1\n",
        "\n",
        "  # counter for the base image selection / image1:\n",
        "  # basically this section runs when we have finished processing a base image and ready to merge the generation crops finally with the base!\n",
        "  if countRepeatPrompts == countOfFacesPerInput[selectInitialImage]: # once we're run through all the crops for this intiial image, we have to move to the next one\n",
        "    selectInitialImage += 1\n",
        "    newMerge = new_image.copy() # temp save for image to be pasted on\n",
        "    # at this point we also create the final result by merging the collected extended images with the input image (image1):\n",
        "    imgPathOutputFin = FRpathProjectFinal + \"final_\" + stringImageNumber.zfill(4) + \"_f\" + stringcountRepeatPrompts.zfill(2) + \".png\" # ex. finished_0001.png\n",
        "\n",
        "    for extendedImg in tempCollectExtendedImg:\n",
        "      newMerge.paste(extendedImg,(0,0), extendedImg)\n",
        "      newMerge = newMerge.copy()\n",
        "      newMerge.save(imgPathOutputFin)\n",
        "    countRepeatPrompts = 0\n",
        "    tempCollectExtendedImg.clear() # get ready for appending the next round of crops\n",
        "  # continue until for loop is done...\n",
        "\n",
        "# ----------------------END FACIAL RECOGNITION P2 -------------------------------\n",
        "# -------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "# Now we copy the final files to Google Drive to back them up:\n",
        "if len(init_imgs_list) != 0:\n",
        "  print(\"Wait while copying to Google Drive...\")\n",
        "  copyAddressOnDrive = \"/content/gdrive/MyDrive/AI/StableDiffusionUpscaleFaces/\" + ProjectName\n",
        "\n",
        "  directoryToCopy1 = FRpathProjectFinal # final images\n",
        "  directoryToCopy2 = FRpathProjectUpscaledINIT # upscaled INIT images\n",
        "  directoryToCopy3 = FRpathProjectExtended # extended crops for photobashing\n",
        "  listOfImagesToCopy1 = [f for f in listdir(directoryToCopy1) if f.endswith(\".png\") and isfile(join(directoryToCopy1, f))]\n",
        "  listOfImagesToCopy1.sort()\n",
        "  listOfImagesToCopy2 = [f for f in listdir(directoryToCopy2) if f.endswith(\".png\") and isfile(join(directoryToCopy2, f))]\n",
        "  listOfImagesToCopy2.sort()\n",
        "  listOfImagesToCopy3 = [f for f in listdir(directoryToCopy3) if f.endswith(\".png\") and isfile(join(directoryToCopy3, f))]\n",
        "  listOfImagesToCopy3.sort()\n",
        "\n",
        "  for index, file in enumerate(listOfImagesToCopy2):\n",
        "    valueSt = str(index+1) \n",
        "    newName = \"upscaledInput_\" + valueSt.zfill(4)\n",
        "    os.rename(os.path.join(directoryToCopy2, file), os.path.join(directoryToCopy2, ''.join([newName, '.png'])))  \n",
        "  listOfImagesToCopy2 = [f for f in listdir(directoryToCopy2) if f.endswith(\".png\") and isfile(join(directoryToCopy2, f))]\n",
        "  listOfImagesToCopy2.sort()\n",
        "\n",
        "  isExist = os.path.exists(copyAddressOnDrive + \"/001\")\n",
        "  if not isExist:\n",
        "      os.makedirs(copyAddressOnDrive + \"/001\")\n",
        "      copyAddressOnDrive = copyAddressOnDrive + \"/001\"\n",
        "  else:\n",
        "    i=1\n",
        "    while os.path.exists(copyAddressOnDrive + \"/\" + str(i).zfill(3)):\n",
        "      i+=1\n",
        "    os.makedirs(copyAddressOnDrive + \"/\" + str(i).zfill(3))\n",
        "    copyAddressOnDrive = copyAddressOnDrive + \"/\" + str(i).zfill(3)\n",
        "\n",
        "  for x in listOfImagesToCopy1:\n",
        "    fullPath = directoryToCopy1 + x\n",
        "    shutil.copy(fullPath, copyAddressOnDrive)\n",
        "\n",
        "  for x in listOfImagesToCopy2:\n",
        "    fullPath = directoryToCopy2 + x\n",
        "    shutil.copy(fullPath, copyAddressOnDrive)\n",
        "\n",
        "  for x in listOfImagesToCopy3:\n",
        "    fullPath = directoryToCopy3 + x\n",
        "    shutil.copy(fullPath, copyAddressOnDrive)\n",
        "\n",
        "  print(\"Please wait for files to copy to Google Drive. Enjoy!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Experimental Gender & Age Detection (study only):\n",
        "#@markdown `note: this script will not do more than study an image and guess the gender/age`<br>\n",
        "#@markdown `Gender detection is fine, but the age detection is way off`<br>\n",
        "#@markdown https://data-flair.training/blogs/image-segmentation-machine-learning/\n",
        "\n",
        "\n",
        "# models are required to be added manually to /content/gad/\n",
        "# models are from here: https://data-flair.training/blogs/python-project-gender-age-detection/\n",
        "# this code was written and taken from here: https://dev.to/ethand91/simple-age-and-gender-detection-using-python-and-opencv-319h\n",
        "\n",
        "\n",
        "# Gender and Age Detection Code Fragment\n",
        "\n",
        "import cv2\n",
        "import math\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Defined the model files\n",
        "pathStart = \"/content/gad/\"\n",
        "FACE_PROTO = pathStart + \"opencv_face_detector.pbtxt\"\n",
        "FACE_MODEL = pathStart + \"opencv_face_detector_uint8.pb\"\n",
        "\n",
        "AGE_PROTO = pathStart + \"age_deploy.prototxt\"\n",
        "AGE_MODEL = pathStart + \"age_net.caffemodel\"\n",
        "\n",
        "GENDER_PROTO = pathStart + \"gender_deploy.prototxt\"\n",
        "GENDER_MODEL = pathStart + \"gender_net.caffemodel\"\n",
        "\n",
        "# Load network\n",
        "FACE_NET = cv2.dnn.readNet(FACE_MODEL, FACE_PROTO)\n",
        "AGE_NET = cv2.dnn.readNet(AGE_MODEL, AGE_PROTO)\n",
        "GENDER_NET = cv2.dnn.readNet(GENDER_MODEL, GENDER_PROTO)\n",
        "\n",
        "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
        "AGE_LIST = [\"(0-2)\", \"(4-6)\", \"(8-12)\", \"(15-20)\", \"(25-32)\", \"(38-43)\", \"(48-53)\", \"(60-100)\"]\n",
        "GENDER_LIST = [\"Male\", \"Female\"]\n",
        "\n",
        "box_padding = 20\n",
        "\n",
        "def get_face_box (net, frame, conf_threshold = 0.7):\n",
        "  frame_copy = frame.copy()\n",
        "  frame_height = frame_copy.shape[0]\n",
        "  frame_width = frame_copy.shape[1]\n",
        "  blob = cv2.dnn.blobFromImage(frame_copy, 1.0, (300, 300), [104, 117, 123], True, False)\n",
        "\n",
        "  net.setInput(blob)\n",
        "  detections = net.forward()\n",
        "  boxes = []\n",
        "\n",
        "  for i in range(detections.shape[2]):\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "\n",
        "    if confidence > conf_threshold:\n",
        "      x1 = int(detections[0, 0, i, 3] * frame_width)\n",
        "      y1 = int(detections[0, 0, i, 4] * frame_height)\n",
        "      x2 = int(detections[0, 0, i, 5] * frame_width)\n",
        "      y2 = int(detections[0, 0, i, 6] * frame_height)\n",
        "      boxes.append([x1, y1, x2, y2])\n",
        "      cv2.rectangle(frame_copy, (x1, y1), (x2, y2), (0, 255, 0), int(round(frame_height / 150)), 8)\n",
        "\n",
        "  return frame_copy, boxes\n",
        "\n",
        "def age_gender_detector (input_path):\n",
        "  image = cv2.imread(input_path)\n",
        "  frame = image.copy()\n",
        "  frame_face, boxes = get_face_box(FACE_NET, frame)\n",
        "\n",
        "  for box in boxes:\n",
        "    face = frame[max(0, box[1] - box_padding):min(box[3] + box_padding, frame.shape[0] - 1), \\\n",
        "      max(0, box[0] - box_padding):min(box[2] + box_padding, frame.shape[1] - 1)]\n",
        "\n",
        "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB = False)\n",
        "    GENDER_NET.setInput(blob)\n",
        "    gender_predictions = GENDER_NET.forward()\n",
        "    gender = GENDER_LIST[gender_predictions[0].argmax()]\n",
        "    print(\"Gender: {}, conf: {:.3f}\".format(gender, gender_predictions[0].max()))\n",
        "\n",
        "    AGE_NET.setInput(blob)\n",
        "    age_predictions = AGE_NET.forward()\n",
        "    age = AGE_LIST[age_predictions[0].argmax()]\n",
        "    print(\"Age: {}, conf: {:.3f}\".format(age, age_predictions[0].max()))\n",
        "\n",
        "    label = \"{},{}\".format(gender, age)\n",
        "    cv2.putText(frame_face, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "  return frame_face\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  inputImageFullPath = \"/content/init_image/a0011.png\" #@param {type:\"string\"}\n",
        "  output = age_gender_detector(inputImageFullPath)\n",
        "  cv2.imwrite(\"output.jpg\", output)\n",
        "  cv2_imshow(output)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1z-Twr9G3A0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C) Upscale from File Browser (Real ESRGAN)"
      ],
      "metadata": {
        "id": "o3R1pX2brMPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "#@markdown `note: run this to upres from selecting file browser`<br>\n",
        "#1) set up the file to load into the ESRGAN:\n",
        "upload_folder = 'upload' # wierd directories that ESRGAN uses\n",
        "result_folder = 'results'\n",
        "\n",
        "if os.path.isdir(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "if os.path.isdir(result_folder):\n",
        "    shutil.rmtree(result_folder)\n",
        "os.mkdir(upload_folder)\n",
        "os.mkdir(result_folder)\n",
        "\n",
        "# upload images\n",
        "# upload = \"/content/1.jpg\" #@param {type:\"string\"}\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "  dst_path = os.path.join(upload_folder, filename)\n",
        "  print(f'move {filename} to {dst_path}')\n",
        "  shutil.copy(filename, dst_path)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------\n",
        "# 2) Core Code to Make ESRGAN Uprez the Image:\n",
        "# if it is out of memory, try to use the `--tile` option\n",
        "# We upsample the image with the scale factor X3.5\n",
        "\n",
        "!python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 2 --face_enhance\n",
        "# Arguments\n",
        "# -n, --model_name: Model names\n",
        "# -i, --input: input folder or image\n",
        "# --outscale: Output scale, can be arbitrary scale factore. \n",
        "# ---------------------------------------------------------------------------------------------\n",
        "\n",
        "input_folder = '/content/stable-diffusion/Real-ESRGAN/upload'\n",
        "result_folder = '/content/stable-diffusion/Real-ESRGAN/results'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n",
        "output_list = sorted(glob.glob(os.path.join(result_folder, '*')))\n",
        "output_list.sort()\n",
        "print(input_list)\n",
        "print(output_list)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L6GCJDkYrXCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown `Download the Upscaled Images as .zip`<br>\n",
        "\n",
        "# Make a zip file of your results to download:\n",
        "from google.colab import files\n",
        "import os\n",
        "file_path = \"/content/stable-diffusion/Real-ESRGAN/results.zip\"\n",
        "if os.path.isfile(file_path):\n",
        "  os.remove(file_path)\n",
        "!zip -r results.zip /content/stable-diffusion/Real-ESRGAN/results\n",
        "files.download('results.zip')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eWGl_J8krgvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <u>Utility:</u> Clear Image Folder"
      ],
      "metadata": {
        "id": "nvEQDBd2cNh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "files = glob.glob('/content/init_image/**/*', recursive=True)\n",
        "\n",
        "for f in files:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "    except OSError as e:\n",
        "        print(\"Error: %s : %s\" % (f, e.strerror))\n",
        "Info = 'run to clear image folder' #@param [\"run to clear image folder\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W84nCpoKVP_o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "UU52ZvES6-1T",
        "0GOr30k9Gh1L",
        "aGldtYOMb5fk",
        "rGlb8rzxcB7F",
        "o8hwXeJhIfJl",
        "B8rTt1lqb3uy",
        "o3R1pX2brMPF",
        "nvEQDBd2cNh7"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bee999d40027416785d4698f7e3e988c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77c264cdfb21453f8994daa2360d6054",
              "IPY_MODEL_a4398d2cf4e94bb08eedd79535485bab",
              "IPY_MODEL_12a9a30f474d44859f7389bed8a7ac0e"
            ],
            "layout": "IPY_MODEL_72269156dce7432cab71dc93b93de244"
          }
        },
        "77c264cdfb21453f8994daa2360d6054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_013f61fbb3044e02956773ac487bb03a",
            "placeholder": "",
            "style": "IPY_MODEL_4806fc725c6c4dfc9efc90a7b27c9167",
            "value": "Downloading vocab.json: 100%"
          }
        },
        "a4398d2cf4e94bb08eedd79535485bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b324e263490544eb8d8fd463a88593b6",
            "max": 961143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71a8c096b78c4fb285790344e3773675",
            "value": 961143
          }
        },
        "12a9a30f474d44859f7389bed8a7ac0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3101b0bb5fb43859dcd0214cf6095f6",
            "placeholder": "",
            "style": "IPY_MODEL_19cc9e6e5e2b48c2b044a3a2eb2c93c5",
            "value": " 939k/939k [00:00&lt;00:00, 2.95MB/s]"
          }
        },
        "72269156dce7432cab71dc93b93de244": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "013f61fbb3044e02956773ac487bb03a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4806fc725c6c4dfc9efc90a7b27c9167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b324e263490544eb8d8fd463a88593b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a8c096b78c4fb285790344e3773675": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3101b0bb5fb43859dcd0214cf6095f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19cc9e6e5e2b48c2b044a3a2eb2c93c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da49cf804f3d40adbab13ec0ae554b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ff40adbbc1a4eb4a286132eee09b029",
              "IPY_MODEL_5463b99d75e54069be942114d54502fb",
              "IPY_MODEL_df23800149dd42a18385a2198bad7846"
            ],
            "layout": "IPY_MODEL_2e61374220f44c279ced59eb1617f4a9"
          }
        },
        "1ff40adbbc1a4eb4a286132eee09b029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e586966c78764abbbf20b0ee12cb1e59",
            "placeholder": "",
            "style": "IPY_MODEL_c7faafbe10734dbbb88568e1fbcae3d5",
            "value": "Downloading merges.txt: 100%"
          }
        },
        "5463b99d75e54069be942114d54502fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d351119c30544b1bdc335250adf6b37",
            "max": 524619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55ebe9d5619d4f379d14ea87a531ef1f",
            "value": 524619
          }
        },
        "df23800149dd42a18385a2198bad7846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeefff6f061a4fa2b65dfb85e359e234",
            "placeholder": "",
            "style": "IPY_MODEL_83499bca191140f28e191bccb48e1809",
            "value": " 512k/512k [00:00&lt;00:00, 914kB/s]"
          }
        },
        "2e61374220f44c279ced59eb1617f4a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e586966c78764abbbf20b0ee12cb1e59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7faafbe10734dbbb88568e1fbcae3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d351119c30544b1bdc335250adf6b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ebe9d5619d4f379d14ea87a531ef1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eeefff6f061a4fa2b65dfb85e359e234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83499bca191140f28e191bccb48e1809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4040a1d581fd432a8ea7f48801e406a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d9a8dcde09640119a7133e71f7b7b06",
              "IPY_MODEL_eec9abc1a09044a29705e657e32694b3",
              "IPY_MODEL_2ef156e0ee7f47efa9025c0aa11a7a61"
            ],
            "layout": "IPY_MODEL_7463f1a3ea674bf688104b934591ae51"
          }
        },
        "7d9a8dcde09640119a7133e71f7b7b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b852044834c2415a99dcf6f67b795101",
            "placeholder": "",
            "style": "IPY_MODEL_7ca9417a57144e26ba00472692022515",
            "value": "Downloading special_tokens_map.json: 100%"
          }
        },
        "eec9abc1a09044a29705e657e32694b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a94f5068da349799d0d8d9014399b56",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_238e275ba7b44f4dabb9e61ba929d839",
            "value": 389
          }
        },
        "2ef156e0ee7f47efa9025c0aa11a7a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f637b6cc5d684720a9ee62c37f1c925b",
            "placeholder": "",
            "style": "IPY_MODEL_e7a55be534164e31b4800460b741659b",
            "value": " 389/389 [00:00&lt;00:00, 12.2kB/s]"
          }
        },
        "7463f1a3ea674bf688104b934591ae51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b852044834c2415a99dcf6f67b795101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ca9417a57144e26ba00472692022515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a94f5068da349799d0d8d9014399b56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "238e275ba7b44f4dabb9e61ba929d839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f637b6cc5d684720a9ee62c37f1c925b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7a55be534164e31b4800460b741659b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a72913ff8ce4b21a87c10b992dd1b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ff886fbf15d43f19460cac8d83bd3b3",
              "IPY_MODEL_3a8f4a589180403e83b2b5b9896d70a1",
              "IPY_MODEL_a4cc559d014b42d3b500e42744bd35b8"
            ],
            "layout": "IPY_MODEL_f427314ae73648a78abaadf8775e52b9"
          }
        },
        "6ff886fbf15d43f19460cac8d83bd3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa55ac49ef4e4f53a3f141f910b25d6f",
            "placeholder": "",
            "style": "IPY_MODEL_d8451f010ae34ecd99001f8497c44084",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "3a8f4a589180403e83b2b5b9896d70a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_838792a8c3ae4dcf8fa40556a8cdc4e2",
            "max": 905,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_633d9d004c0e441484331ada63fa7e02",
            "value": 905
          }
        },
        "a4cc559d014b42d3b500e42744bd35b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8681474f4f342a5baa6526cf248a663",
            "placeholder": "",
            "style": "IPY_MODEL_50343766638c47e2b3b84d42aa22e9d7",
            "value": " 905/905 [00:00&lt;00:00, 23.3kB/s]"
          }
        },
        "f427314ae73648a78abaadf8775e52b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa55ac49ef4e4f53a3f141f910b25d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8451f010ae34ecd99001f8497c44084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "838792a8c3ae4dcf8fa40556a8cdc4e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "633d9d004c0e441484331ada63fa7e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8681474f4f342a5baa6526cf248a663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50343766638c47e2b3b84d42aa22e9d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df2fbb7104ee4f55bbe3c4271418d00b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_997bad17e95a422dbc6db5b04f934fc3",
              "IPY_MODEL_b0576877c08a48d3ae9bfa625b1d98f9",
              "IPY_MODEL_90db2ecf78f84f0ba646797ab098bbd5"
            ],
            "layout": "IPY_MODEL_d8071a44828d4b279e2570016a95c5d3"
          }
        },
        "997bad17e95a422dbc6db5b04f934fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_311303c6a56c4bbf9c593db95818edc4",
            "placeholder": "",
            "style": "IPY_MODEL_076646f26d774e7c99acd2d77721d3bd",
            "value": "Downloading config.json: 100%"
          }
        },
        "b0576877c08a48d3ae9bfa625b1d98f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24ef54aa93224d2ca8a152918443101a",
            "max": 4409,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce299766c2374cb7baeb4c813e1289ec",
            "value": 4409
          }
        },
        "90db2ecf78f84f0ba646797ab098bbd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d47ebcc69def47fea5c14d33736f7037",
            "placeholder": "",
            "style": "IPY_MODEL_672f185e14c047c399e19a5384206c7d",
            "value": " 4.31k/4.31k [00:00&lt;00:00, 130kB/s]"
          }
        },
        "d8071a44828d4b279e2570016a95c5d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311303c6a56c4bbf9c593db95818edc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "076646f26d774e7c99acd2d77721d3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24ef54aa93224d2ca8a152918443101a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce299766c2374cb7baeb4c813e1289ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d47ebcc69def47fea5c14d33736f7037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "672f185e14c047c399e19a5384206c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5977579f7b4c48e5852761ccdb69fef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9cd8451d45a421f9117455d56dd43e9",
              "IPY_MODEL_caab461d98764eafb6688ee0556e04ee",
              "IPY_MODEL_4a0b799401a248eb874ac1a4b9c8724e"
            ],
            "layout": "IPY_MODEL_6396e08c4e5e42d8a96e630b1619581c"
          }
        },
        "b9cd8451d45a421f9117455d56dd43e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e8dac7d9dcb4341999b44cf3d26a06d",
            "placeholder": "",
            "style": "IPY_MODEL_b83b8f09613c4c10bbbfab63b52e6d7a",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "caab461d98764eafb6688ee0556e04ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8254f3e8d9247f4bca280cce84127bf",
            "max": 1710671599,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7639c1bf1344380823ed5750d73db2e",
            "value": 1710671599
          }
        },
        "4a0b799401a248eb874ac1a4b9c8724e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3292434dee9462e93ed396085eb8134",
            "placeholder": "",
            "style": "IPY_MODEL_09a241563dd94966a4d1a7cc7faed52b",
            "value": " 1.59G/1.59G [00:47&lt;00:00, 42.2MB/s]"
          }
        },
        "6396e08c4e5e42d8a96e630b1619581c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e8dac7d9dcb4341999b44cf3d26a06d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b83b8f09613c4c10bbbfab63b52e6d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8254f3e8d9247f4bca280cce84127bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7639c1bf1344380823ed5750d73db2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3292434dee9462e93ed396085eb8134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09a241563dd94966a4d1a7cc7faed52b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}